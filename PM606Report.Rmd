---
title: "Analysis of Environmental Exposures, Diet, and Metabolomics on Child BMI Z-Score"
author: "Allison Louie"
date: "2024-08-05"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_folding: hide
---

```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(summarytools)
library(kableExtra)
library(tidyverse)
library(Biobase)
library(plotly)
library(profvis)
library(corrplot)
library(table1)
library(tableone)
library(randomForest)
library(caret)
library(rpart)
library(rpart.plot)
library(gbm)
library(grpreg)
library(glmnet)
library(cluster)
library(factoextra)
library(Hmisc)
library(tidyr)
library(grplasso)
options(knitr.table.format = "html")
knitr::opts_chunk$set(echo = TRUE)
work.dir <- here::here()
old.warn <- getOption("warn")
options(warn=-1)
```

# Analysis of Environmental Exposures, Diet, and Metabolomics on Child BMI Z-Score

# Abstract

**Background**: Identifying the key predictors of childhood Body Mass Index (BMI) is crucial for informing effective health interventions, given the significant impacts of diet, chemical exposures, and metabolomic profiles on health outcomes. Understanding the multifactorial influences is crucial for developing effective interventions and policies.

**Objective**: This study aims to predict childhood BMI Z-scores by integrating comprehensive data on postnatal diet, chemical exposures, and serum metabolomics, using advanced statistical models.

**Methods**: Data from the HELIX study, including 1301 mother-child pairs aged 6-11 years, were analyzed. Data was split into training (70%) and test (30%) sets. Various modeling approaches, including LASSO, ridge, elastic net, decision tree, random forest, and gradient boosting machine (GBM), with 10-fold cross-validation used to evaluate model performance and ensure robustness. Models were evaluated based on their Root Mean Squared Error (RMSE) and the significance of the predictors identified.

**Results**: The results demonstrate that models incorporating a comprehensive set of variables (diet, chemicals, metabolomics, and covariates) consistently outperformed those with fewer variables. The inclusion of metabolomic data significantly improved model performance. GBM model with all variables demonstrated the best performance with a cross-validated RMSE of 0.9537, followed closely by the group LASSO model with an RMSE of 0.8748. Key predictors identified across the models included age and sex, specific metabolites such as metab_161 (coefficient 3.5751), metab_95 (1.3657), and metab_160 (-3.2241), along with chemical exposures like polychlorinated biphenyl-170 (hs_pcb170_cadj_Log2). The random forest model also showed strong performance with an RMSE of 1.0047.

**Conclusion**: Integrating demographic, dietary, chemical, and metabolomic data provides robust predictions for BMI in children. Advanced machine learning techniques, particularly GBM and Random Forest, significantly enhance predictive performance. These findings underscore the importance of utilizing diverse data types to understand and address childhood obesity effectively.

# Introduction

## Background

The impact of postnatal environmental exposures on childhood health outcomes is critical for understanding long-term health implications in children. Understanding the multifactorial influences on childhood BMI is crucial for developing effective interventions and policies to combat this issue. This study aims to understand the impact of postnatal environmental exposures, dietary habits, and covariates on the BMI Z-score of children aged 6-11 years.

Research indicates that postnatal exposure to endocrine-disrupting chemicals (EDCs) such as phthalates, bisphenol A (BPA), and polychlorinated biphenyls (PCBs) can significantly influence body weight and metabolic health ([Junge et al., 2018](https://clinicalepigeneticsjournal.biomedcentral.com/articles/10.1186/s13148-018-0478-z)). These chemicals, commonly found in household products and absorbed through dietary intake, are linked to detrimental effects on metabolic health in children. This hormonal interference can lead to an increased body mass index (BMI) in children, suggesting a potential pathway through which exposure to these chemicals contributes to the development of obesity.

The study by [Harley et al. (2013)](https://pubmed.ncbi.nlm.nih.gov/23416456/) investigates the association between prenatal and postnatal Bisphenol A (BPA) exposure and various body composition metrics in children aged 9 years from the CHAMACOS cohort. The study found that higher prenatal BPA exposure was linked to a decrease in BMI and body fat percentages in girls but not boys, suggesting sex-specific effects. Conversely, BPA levels measured at age 9 were positively associated with increased adiposity in both genders, highlighting the different impacts of exposure timing on childhood development.

A [2016 study by Caspersen et al.](https://www.sciencedirect.com/science/article/abs/pii/S0013935115301821?via%3Dihub) investigated the determinants of plasma levels of polychlorinated biphenyls (PCBs), brominated flame retardants, and organochlorine pesticides in pregnant women and 3-year-old children. In pregnant women, significant predictors of increased plasma concentrations of persistent organic pollutants (POPs) included age, low parity, and low pre-pregnant BMI. With the 3-year-olds, prolonged breastfeeding duration was a major determinant of increased POP concentrations. Diet was a significant predictor of PCB levels in children. By examining environmental exposures and their influence on BMI, this project aims to extend the understanding of how specific dietary patterns and chemical exposures during early childhood contribute to BMI variations, aligning with the findings that diet significantly predicts POP levels in children.

These studies collectively illustrate the critical role of environmental exposures in shaping metabolic health outcomes in children, highlighting the necessity for ongoing research and policy intervention to mitigate these risks.

## Hypothesis

How are postnatal environmental exposures, specifically those found in household products and dietary intake, along with specific serum metabolomics profiles, associated with the BMI Z-score of children aged 6-11 years?  Specifically, which metabolites associated with chemical exposures and dietary patterns serve as biomarkers for the risk of developing obesity, while controlling for age and other relevant covariates?

# Methods

## Data Description

This study utilizes data from the subcohort of 1301 mother-child pairs in the HELIX study, who are which aged 6-11 years for whom complete exposure and outcome data were available. Exposure data included detailed dietary records after pregnancy and concentrations of various chemicals in child blood samples. There are categorical and numerical variables, which will include both demographic details and biochemical measurements. This dataset allows for robust statistical analysis to identify potential associations between chemical exposure and changes in BMI Z-scores, considering confounding factors such as age, gender, and socioeconomic status. There are no missing data so there is not need to impute the information. Child BMI Z-scores were calculated based on WHO growth standards.

In terms of adding the metabolomic data with the rest of the dataset, 103 (about 8% of the data) values were excluded. Some data entries had no information when combined with the metabolomic data and therefore could not be imputed with the mean or median. While this may pose as a risk, including rows with missing values can lead to inaccurate or biased results. This is relevant since many statistical methods and machine learning algorithms require complete data to function correctly. The proportion of missing data (8%) is relatively small, making the exclusion of these rows a feasible approach. This minimizes the loss of valuable information while still maintaining a dataset of sufficient size for meaningful analysis.

```{r load data and codebook, echo=TRUE}
load("/Users/allison/Library/CloudStorage/GoogleDrive-aflouie@usc.edu/My Drive/HELIX_data/HELIX.RData")
filtered_chem_diet <- codebook %>%
  filter(domain %in% c("Chemicals", "Lifestyles") & period == "Postnatal" & subfamily != "Allergens")

# specific covariates
filtered_covariates <- codebook %>%
  filter(domain == "Covariates" & 
         variable_name %in% c("ID", "e3_sex_None", "e3_yearbir_None", "hs_child_age_None"))

#specific phenotype variables
filtered_phenotype <- codebook %>%
  filter(domain == "Phenotype" & 
         variable_name %in% c("hs_zbmi_who"))

# combining all necessary variables together
combined_codebook <- bind_rows(filtered_chem_diet, filtered_covariates, filtered_phenotype)
```

```{r metabolomic data added, warning = FALSE}
load("/Users/allison/Library/CloudStorage/GoogleDrive-aflouie@usc.edu/My Drive/HELIX_data/metabol_serum.RData")
metabol_serum_transposed <- as.data.frame(t(metabol_serum.d))
metabol_serum_transposed$ID <- as.integer(rownames(metabol_serum_transposed))

# add the ID column to the first position
metabol_serum_transposed <- metabol_serum_transposed[, c("ID", setdiff(names(metabol_serum_transposed), "ID"))]

# ID is the first column, and the layout is preserved
kable(head(metabol_serum_transposed), align = "c", digits = 2, format = "pipe")

# specific covariates
load("/Users/allison/Library/CloudStorage/GoogleDrive-aflouie@usc.edu/My Drive/HELIX_data/HELIX.RData")
filtered_chem_diet <- codebook %>%
  filter(domain %in% c("Chemicals", "Lifestyles") & period == "Postnatal" & subfamily != "Allergens")

# specific covariates
filtered_covariates <- codebook %>%
  filter(domain == "Covariates" & 
         variable_name %in% c("ID", "e3_sex_None", "e3_yearbir_None", "hs_child_age_None"))

#specific phenotype variables
filtered_phenotype <- codebook %>%
  filter(domain == "Phenotype" & 
         variable_name %in% c("hs_zbmi_who"))

# combining all necessary variables together
combined_codebook <- bind_rows(filtered_chem_diet, filtered_covariates, filtered_phenotype)

outcome_BMI <- phenotype %>% 
  dplyr::select(hs_zbmi_who)

outcome_and_cov <- cbind(covariates, outcome_BMI)
outcome_and_cov <- outcome_and_cov[, !duplicated(colnames(outcome_and_cov))]
outcome_and_cov <- outcome_and_cov %>%
  dplyr::select(ID, hs_child_age_None, e3_sex_None, e3_yearbir_None, hs_zbmi_who)
#the full chemicals list
chemicals_specific <- c(
  "hs_cd_c_Log2",
  "hs_co_c_Log2",
  "hs_cs_c_Log2",
  "hs_cu_c_Log2",
  "hs_hg_c_Log2",
  "hs_mo_c_Log2",
  "hs_pb_c_Log2",
  "hs_dde_cadj_Log2",
  "hs_pcb153_cadj_Log2",
  "hs_pcb170_cadj_Log2",
  "hs_dep_cadj_Log2",
  "hs_pbde153_cadj_Log2",
  "hs_pfhxs_c_Log2",
  "hs_pfoa_c_Log2",
  "hs_pfos_c_Log2",
  "hs_prpa_cadj_Log2",
  "hs_mbzp_cadj_Log2",
  "hs_mibp_cadj_Log2",
  "hs_mnbp_cadj_Log2"
)

#postnatal diet for child
postnatal_diet <- c(
  "h_bfdur_Ter",
  "hs_bakery_prod_Ter",
  "hs_dairy_Ter",
  "hs_fastfood_Ter",
  "hs_org_food_Ter",
  "hs_readymade_Ter",
  "hs_total_bread_Ter",
  "hs_total_fish_Ter",
  "hs_total_fruits_Ter",
  "hs_total_lipids_Ter",
  "hs_total_potatoes_Ter",
  "hs_total_sweets_Ter",
  "hs_total_veg_Ter"
)

covariates_selected <- c("hs_child_age_None", "e3_sex_None", "e3_yearbir_None")

all_columns <- c(chemicals_specific, postnatal_diet)
extracted_exposome <- exposome %>% dplyr::select(all_of(all_columns))

selected_id_data <- cbind(outcome_and_cov, extracted_exposome)

# ID is the common identifier in both datasets
combined_data <- merge(selected_id_data, metabol_serum_transposed, by = "ID", all = TRUE)

selected_metabolomics_data <- combined_data %>% dplyr::select(-c(ID))

# the number of NAs in each column
missing_summary <- sapply(selected_metabolomics_data, function(x) sum(is.na(x)))

missing_summary_df <- data.frame(
  Variable = names(missing_summary),
  Missing_Count = missing_summary,
  Missing_Percentage = (missing_summary / nrow(selected_metabolomics_data)) * 100
)

missing_summary_df <- missing_summary_df %>% filter(Missing_Count > 0)
missing_summary_df

selected_metabolomics_data <- selected_metabolomics_data %>% na.omit() #gets rid of 103 rows from original (8% of original data) since there are areas that have NA throughout metabol_#
```

## Data Summary for Exposures, Covariates, and Outcome {.tabset}

### Data Summary Exposures: Dietary

These variables were categorized into tertiles to assess the impact of different levels of dietary intake on BMI Z-scores. The dietary intake variables included:

-   `h_bfdur_Ter`: Duration of breastfeeding

-   `hs_bakery_prod_Ter`: Intake of bakery products

-   `hs_break_cer_Ter`: Intake of breakfast cereals

-   `hs_dairy_Ter`: Intake of dairy products

-   `hs_fastfood_Ter`: Intake of fast food

-   `hs_org_food_Ter`: Intake of organic food

-   `hs_proc_meat_Ter`: Intake of processed meat

-   `hs_total_fish_Ter`: Intake of total fish

-   `hs_total_fruits_Ter`: Intake of total fruits

-   `hs_total_lipids_Ter`: Intake of total lipids

-   `hs_total_sweets_Ter`: Intake of total sweets

-   `hs_total_veg_Ter`: Intake of total vegetables

```{r Lifestyles summary, attr.output='style="max-height: 100px;"',}
# specific lifestyle exposures
dietary_exposures <- c(
  "h_bfdur_Ter",
  "hs_bakery_prod_Ter",
  "hs_break_cer_Ter",
  "hs_dairy_Ter",
  "hs_fastfood_Ter",
  "hs_org_food_Ter",
  "hs_proc_meat_Ter",
  "hs_total_fish_Ter",
  "hs_total_fruits_Ter",
  "hs_total_lipids_Ter",
  "hs_total_sweets_Ter",
  "hs_total_veg_Ter"
)

dietary_exposome <- dplyr::select(exposome, all_of(dietary_exposures))
summarytools::view(dfSummary(dietary_exposome, style = 'grid', plain.ascii = FALSE, valid.col = FALSE, headings = FALSE), method = "render")
```

### Data Summary Exposures: Chemicals

This study included a comprehensive assessment of various chemical exposures to understand their impact on childhood BMI Z-scores. The specific chemical exposures selected for analysis were as follows:

-   `hs_cd_c_Log2`: Cadmium concentration

-   `hs_co_c_Log2`: Cobalt concentration

-   `hs_cs_c_Log2`: Cesium concentration

-   `hs_cu_c_Log2`: Copper concentration

-   `hs_hg_c_Log2`: Mercury concentration

-   `hs_mo_c_Log2`: Molybdenum concentration

-   `hs_pb_c_Log2`: Lead concentration

-   `hs_dde_cadj_Log2`: DDE (a breakdown product of DDT) concentration, adjusted

-   `hs_pcb153_cadj_Log2`: PCB 153 concentration, adjusted

-   `hs_pcb170_cadj_Log2`: PCB 170 concentration, adjusted

-   `hs_dep_cadj_Log2`: DEP (Diethyl phthalate) concentration, adjusted

-   `hs_pbde153_cadj_Log2`: PBDE 153 (Polybrominated diphenyl ether) concentration, adjusted

-   `hs_pfhxs_c_Log2`: PFHxS (Perfluorohexane sulfonic acid) concentration

-   `hs_pfoa_c_Log2`: PFOA (Perfluorooctanoic acid) concentration

-   `hs_pfos_c_Log2`: PFOS (Perfluorooctane sulfonic acid) concentration

-   `hs_prpa_cadj_Log2`: PRPA (Propargyl alcohol) concentration, adjusted

-   `hs_mbzp_cadj_Log2`: MBzP (Mono-benzyl phthalate) concentration, adjusted

-   `hs_mibp_cadj_Log2`: MiBP (Mono-isobutyl phthalate) concentration, adjusted

-   `hs_mnbp_cadj_Log2`: MnBP (Mono-n-butyl phthalate) concentration, adjusted

```{r Chemicals summary, attr.output='style="max-height: 100px;"',}
# specific chemical exposures
chemical_exposures <- c(
  "hs_cd_c_Log2",
  "hs_co_c_Log2",
  "hs_cs_c_Log2",
  "hs_cu_c_Log2",
  "hs_hg_c_Log2",
  "hs_mo_c_Log2",
  "hs_pb_c_Log2",
  "hs_dde_cadj_Log2",
  "hs_pcb153_cadj_Log2",
  "hs_pcb170_cadj_Log2",
  "hs_dep_cadj_Log2",
  "hs_pbde153_cadj_Log2",
  "hs_pfhxs_c_Log2",
  "hs_pfoa_c_Log2",
  "hs_pfos_c_Log2",
  "hs_prpa_cadj_Log2",
  "hs_mbzp_cadj_Log2",
  "hs_mibp_cadj_Log2",
  "hs_mnbp_cadj_Log2"
)

chemical_exposome <- dplyr::select(exposome, all_of(chemical_exposures))
summarytools::view(dfSummary(chemical_exposome, style = 'grid', plain.ascii = FALSE, valid.col = FALSE, headings = FALSE), method = "render")
```

### Data Summary Covariates

Covariates were selected on its impact with the postnatal nature of the child. The only exception is sex and year of birth, which was considered as pregnancy. This were used since there are differences amongst gender as well as depending on the child's age and when they are born.

```{r covariates, warning=FALSE}
# Specified covariates
specific_covariates <- c(
  "e3_sex_None", 
  "e3_yearbir_None",
  "hs_child_age_None"
)

covariate_data <- dplyr::select(covariates, all_of(specific_covariates))
summarytools::view(dfSummary(covariate_data, style = 'grid', plain.ascii = FALSE, valid.col = FALSE, headings = FALSE), method = "render")
```

### Data Summary Outcome: Phenotype

The primary outcome of interest in this analysis is the Body Mass Index (BMI) Z-score of children, which is a measure of relative weight adjusted for child age and sex. The BMI Z-score provides a standardized way to compare a child's BMI with a reference population

```{r phenotype summary}
outcome_BMI <- phenotype %>% 
  dplyr::select(hs_zbmi_who)
summarytools::view(dfSummary(outcome_BMI, style = 'grid', plain.ascii = FALSE, valid.col = FALSE, headings = FALSE), method = "render")
```

## Descriptive Tables

### By Gender

```{r combining all data, warning=FALSE}
combined_data <- cbind(covariate_data, dietary_exposome, chemical_exposome, outcome_BMI)

combined_data <- combined_data[, !duplicated(colnames(combined_data))]

# sex variable to a factor for stratification
combined_data$e3_sex_None <- as.factor(combined_data$e3_sex_None)
levels(combined_data$e3_sex_None) <- c("Male", "Female")

render_cont <- function(x) {
  with(stats.default(x), sprintf("%0.2f (%0.2f)", MEAN, SD))
}

render_cat <- function(x) {
  c("", sapply(stats.default(x), function(y) with(y, sprintf("%d (%0.1f %%)", FREQ, PCT))))
}

table1_formula <- ~ 
  hs_child_age_None + e3_yearbir_None + 
  hs_zbmi_who +
  h_bfdur_Ter + hs_bakery_prod_Ter + hs_break_cer_Ter + hs_dairy_Ter + hs_fastfood_Ter + hs_org_food_Ter +
  hs_proc_meat_Ter +
  hs_total_fish_Ter + hs_total_fruits_Ter + hs_total_lipids_Ter + hs_total_sweets_Ter + hs_total_veg_Ter +
  hs_cd_c_Log2 + hs_co_c_Log2 + hs_cs_c_Log2 + hs_cu_c_Log2 +
  hs_hg_c_Log2 + hs_mo_c_Log2 + hs_dde_cadj_Log2 + hs_pcb153_cadj_Log2 +
  hs_pcb170_cadj_Log2 + hs_dep_cadj_Log2 + hs_pbde153_cadj_Log2 +
  hs_pfhxs_c_Log2 + hs_pfoa_c_Log2 + hs_pfos_c_Log2 + hs_prpa_cadj_Log2 +
  hs_mbzp_cadj_Log2 + hs_mibp_cadj_Log2 + hs_mnbp_cadj_Log2 | e3_sex_None

table1(
  table1_formula,
  data = combined_data,
  render.continuous = render_cont,
  render.categorical = render_cat,
  overall = TRUE,
  topclass = "Rtable1-shade"
)
```

### By Year of Birth

```{r, warning = FALSE}
combined_data$e3_yearbir_None <- as.factor(combined_data$e3_yearbir_None)

render_cont <- function(x) {
  with(stats.default(x), sprintf("%0.2f (%0.2f)", MEAN, SD))
}

render_cat <- function(x) {
  c("", sapply(stats.default(x), function(y) with(y, sprintf("%d (%0.1f %%)", FREQ, PCT))))
}

table1_formula_year <- ~ 
  hs_child_age_None + e3_sex_None + 
  hs_zbmi_who +
  h_bfdur_Ter + hs_bakery_prod_Ter + hs_break_cer_Ter + hs_dairy_Ter + hs_fastfood_Ter + hs_org_food_Ter +
  hs_proc_meat_Ter +
  hs_total_fish_Ter + hs_total_fruits_Ter + hs_total_lipids_Ter + hs_total_sweets_Ter + hs_total_veg_Ter +
  hs_cd_c_Log2 + hs_co_c_Log2 + hs_cs_c_Log2 + hs_cu_c_Log2 +
  hs_hg_c_Log2 + hs_mo_c_Log2 + hs_dde_cadj_Log2 + hs_pcb153_cadj_Log2 +
  hs_pcb170_cadj_Log2 + hs_dep_cadj_Log2 + hs_pbde153_cadj_Log2 +
  hs_pfhxs_c_Log2 + hs_pfoa_c_Log2 + hs_pfos_c_Log2 + hs_prpa_cadj_Log2 +
  hs_mbzp_cadj_Log2 + hs_mibp_cadj_Log2 + hs_mnbp_cadj_Log2 | e3_yearbir_None

table1(
  table1_formula_year,
  data = combined_data,
  render.continuous = render_cont,
  render.categorical = render_cat,
  overall = TRUE,
  topclass = "Rtable1-shade"
)
```

## Variable Selection

When selecting variables, elastic net will be applied into the available diet and chemical variables in the HELIX data. Elastic net was utilized for variable selection and further analysis.

```{r interested data, warning=FALSE}
outcome_cov <- cbind(covariate_data, outcome_BMI)
outcome_cov <- outcome_cov[, !duplicated(colnames(outcome_cov))]
#the full chemicals list
chemicals_full <- c(
  "hs_as_c_Log2",
  "hs_cd_c_Log2",
  "hs_co_c_Log2",
  "hs_cs_c_Log2",
  "hs_cu_c_Log2",
  "hs_hg_c_Log2",
  "hs_mn_c_Log2",
  "hs_mo_c_Log2",
  "hs_pb_c_Log2",
  "hs_tl_cdich_None",
  "hs_dde_cadj_Log2",
  "hs_ddt_cadj_Log2",
  "hs_hcb_cadj_Log2",
  "hs_pcb118_cadj_Log2",
  "hs_pcb138_cadj_Log2",
  "hs_pcb153_cadj_Log2",
  "hs_pcb170_cadj_Log2",
  "hs_pcb180_cadj_Log2",
  "hs_dep_cadj_Log2",
  "hs_detp_cadj_Log2",
  "hs_dmdtp_cdich_None",
  "hs_dmp_cadj_Log2",
  "hs_dmtp_cadj_Log2",
  "hs_pbde153_cadj_Log2",
  "hs_pbde47_cadj_Log2",
  "hs_pfhxs_c_Log2",
  "hs_pfna_c_Log2",
  "hs_pfoa_c_Log2",
  "hs_pfos_c_Log2",
  "hs_pfunda_c_Log2",
  "hs_bpa_cadj_Log2",
  "hs_bupa_cadj_Log2",
  "hs_etpa_cadj_Log2",
  "hs_mepa_cadj_Log2",
  "hs_oxbe_cadj_Log2",
  "hs_prpa_cadj_Log2",
  "hs_trcs_cadj_Log2",
  "hs_mbzp_cadj_Log2",
  "hs_mecpp_cadj_Log2",
  "hs_mehhp_cadj_Log2",
  "hs_mehp_cadj_Log2",
  "hs_meohp_cadj_Log2",
  "hs_mep_cadj_Log2",
  "hs_mibp_cadj_Log2",
  "hs_mnbp_cadj_Log2",
  "hs_ohminp_cadj_Log2",
  "hs_oxominp_cadj_Log2",
  "hs_cotinine_cdich_None",
  "hs_globalexp2_None"
)

#postnatal diet for child
postnatal_diet <- c(
  "h_bfdur_Ter",
  "hs_bakery_prod_Ter",
  "hs_beverages_Ter",
  "hs_break_cer_Ter",
  "hs_caff_drink_Ter",
  "hs_dairy_Ter",
  "hs_fastfood_Ter",
  "hs_org_food_Ter",
  "hs_proc_meat_Ter",
  "hs_readymade_Ter",
  "hs_total_bread_Ter",
  "hs_total_cereal_Ter",
  "hs_total_fish_Ter",
  "hs_total_fruits_Ter",
  "hs_total_lipids_Ter",
  "hs_total_meat_Ter",
  "hs_total_potatoes_Ter",
  "hs_total_sweets_Ter",
  "hs_total_veg_Ter",
  "hs_total_yog_Ter"
)

chemicals_columns <- c(chemicals_full)
all_chemicals <- exposome %>% dplyr::select(all_of(chemicals_columns))

diet_columns <- c(postnatal_diet)
all_diet <- exposome %>% dplyr::select(all_of(diet_columns))

all_columns <- c(chemicals_full, postnatal_diet)
extracted_exposome <- exposome %>% dplyr::select(all_of(all_columns))

chemicals_outcome_cov <- cbind(outcome_cov, all_chemicals)

diet_outcome_cov <- cbind(outcome_cov, all_diet)

interested_data <- cbind(outcome_cov, extracted_exposome)
head(interested_data)
```

### Chemicals Data

Chemicals will be analyzed for the best variables using enet methods.

```{r chem train/test, warning=FALSE}
# train/test 70-30
set.seed(101)
train_indices <- sample(seq_len(nrow(chemicals_outcome_cov)), size = floor(0.7 * nrow(interested_data)))
test_indices <- setdiff(seq_len(nrow(chemicals_outcome_cov)), train_indices)

x_train <- as.matrix(chemicals_outcome_cov[train_indices, setdiff(names(chemicals_outcome_cov), "hs_zbmi_who")])
y_train <- chemicals_outcome_cov$hs_zbmi_who[train_indices]

x_test <- as.matrix(chemicals_outcome_cov[test_indices, setdiff(names(chemicals_outcome_cov), "hs_zbmi_who")])
y_test <- chemicals_outcome_cov$hs_zbmi_who[test_indices]

x_train_chemicals_only <- as.matrix(chemicals_outcome_cov[train_indices, chemicals_full])
x_test_chemicals_only <- as.matrix(chemicals_outcome_cov[test_indices, chemicals_full])
```

```{r enet, warning=FALSE}
# ELASTIC NET
fit_without_covariates_train <- cv.glmnet(x_train_chemicals_only, y_train, alpha = 0.5, family = "gaussian")
fit_without_covariates_test <- predict(fit_without_covariates_train, s = "lambda.min", newx = x_test_chemicals_only)
test_mse_without_covariates <- mean((y_test - fit_without_covariates_test)^2)
test_rmse_without_covariates <- sqrt(test_mse_without_covariates)
plot(fit_without_covariates_train, xvar = "lambda", main = "Coefficients Path (Without Covariates)")

best_lambda <- fit_without_covariates_train$lambda.min  # lambda that minimizes the MSE
coef(fit_without_covariates_train, s = best_lambda)

cat("Model without Covariates - Test RMSE:", test_rmse_without_covariates)
```

```{r selected chemicals, warning=FALSE}
#selected chemicals that were noted in enet
chemicals_selected <- c(
  "hs_cd_c_Log2",
  "hs_co_c_Log2",
  "hs_cs_c_Log2",
  "hs_cu_c_Log2",
  "hs_hg_c_Log2",
  "hs_mo_c_Log2",
  "hs_pb_c_Log2",
  "hs_dde_cadj_Log2",
  "hs_pcb153_cadj_Log2",
  "hs_pcb170_cadj_Log2",
  "hs_dep_cadj_Log2",
  "hs_detp_cadj_Log2",
  "hs_pbde153_cadj_Log2",
  "hs_pfhxs_c_Log2",
  "hs_pfoa_c_Log2",
  "hs_pfos_c_Log2",
  "hs_mepa_cadj_Log2",
  "hs_oxbe_cadj_Log2",
  "hs_prpa_cadj_Log2",
  "hs_mbzp_cadj_Log2",
  "hs_mibp_cadj_Log2",
  "hs_mnbp_cadj_Log2")
```

The features for chemicals were selected due to the feature selections of elastic net. LASSO might simplify the dimensionality, so elastic net was chosen since feature importance is uncertain.

### Postnatal Diet Data

Like with the chemical variables, the postnatal diet of children will be analyzed for the best variables using the regression methods.

```{r diet test/train, warning=FALSE}
# train/test
set.seed(101)  
train_indices <- sample(seq_len(nrow(diet_outcome_cov)), size = floor(0.7 * nrow(diet_outcome_cov)))
test_indices <- setdiff(seq_len(nrow(diet_outcome_cov)), train_indices)

diet_data <- diet_outcome_cov[, postnatal_diet]
x_diet_train <- model.matrix(~ . + 0, data = diet_data[train_indices, ])  
x_diet_test <- model.matrix(~ . + 0, data = diet_data[test_indices, ])  

covariates <- diet_outcome_cov[, c("e3_sex_None", "e3_yearbir_None", "hs_child_age_None")]
x_covariates_train <- model.matrix(~ . + 0, data = covariates[train_indices, ]) 
x_covariates_test <- model.matrix(~ . + 0, data = covariates[test_indices, ])

x_full_train <- cbind(x_diet_train, x_covariates_train)
x_full_test <- cbind(x_diet_test, x_covariates_test)

x_full_train[is.na(x_full_train)] <- 0
x_full_test[is.na(x_full_test)] <- 0
x_diet_train[is.na(x_diet_train)] <- 0
x_diet_test[is.na(x_diet_test)] <- 0

y_train <- as.numeric(diet_outcome_cov$hs_zbmi_who[train_indices])
y_test <- as.numeric(diet_outcome_cov$hs_zbmi_who[test_indices])
```

```{r diet enet, warning=FALSE}
fit_without_covariates <- cv.glmnet(x_diet_train, y_train, alpha = 0.5, family = "gaussian")
fit_without_covariates

plot(fit_without_covariates, xvar = "lambda", main = "Coefficient Path (Without Covariates)")

best_lambda <- fit_without_covariates$lambda.min  # lambda that minimizes the MSE
coef(fit_without_covariates, s = best_lambda)

predictions_without_covariates <- predict(fit_without_covariates, s = "lambda.min", newx = x_diet_test)
mse_without_covariates <- mean((y_test - predictions_without_covariates)^2)
rmse_without_covariates <- sqrt(mse_without_covariates)
cat("Model without Covariates - Test RMSE:", rmse_without_covariates)
```

```{r important data diet, warning=FALSE}
#selected diets that were noted in enet
diet_selected <- c(
  "h_bfdur_Ter",
  "hs_bakery_prod_Ter",
  "hs_break_cer_Ter",
  "hs_dairy_Ter",
  "hs_fastfood_Ter",
  "hs_org_food_Ter",
  "hs_proc_meat_Ter",
  "hs_total_fish_Ter",
  "hs_total_fruits_Ter",
  "hs_total_lipids_Ter",
  "hs_total_sweets_Ter",
  "hs_total_veg_Ter"
)
```

## Statistical Models

To analyze the impact of dietary, chemical, and demographic variables on BMI Z-scores, various statistical models were employed, each chosen for their unique strengths in handling different aspects of the data. These models were analyzed and performed with a 70-30% split in order to train models and predict the performance. Using a 10-fold cross validation was also applied to avoid overfitting and offer a reliable measure of a model's predictive power and generalizability.

```{r selected data chemicals, warning=FALSE}
#selected chemicals that were noted in enet
chemicals_selected <- c(
  "hs_cd_c_Log2",
  "hs_co_c_Log2",
  "hs_cs_c_Log2",
  "hs_cu_c_Log2",
  "hs_hg_c_Log2",
  "hs_mo_c_Log2",
  "hs_pb_c_Log2",
  "hs_dde_cadj_Log2",
  "hs_pcb153_cadj_Log2",
  "hs_pcb170_cadj_Log2",
  "hs_dep_cadj_Log2",
  "hs_detp_cadj_Log2",
  "hs_pbde153_cadj_Log2",
  "hs_pfhxs_c_Log2",
  "hs_pfoa_c_Log2",
  "hs_pfos_c_Log2",
  "hs_mepa_cadj_Log2",
  "hs_oxbe_cadj_Log2",
  "hs_prpa_cadj_Log2",
  "hs_mbzp_cadj_Log2",
  "hs_mibp_cadj_Log2",
  "hs_mnbp_cadj_Log2")
```

```{r selected data diet, warning=FALSE}
#selected diets that were noted in enet
diet_selected <- c(
  "h_bfdur_Ter",
  "hs_bakery_prod_Ter",
  "hs_break_cer_Ter",
  "hs_dairy_Ter",
  "hs_fastfood_Ter",
  "hs_org_food_Ter",
  "hs_proc_meat_Ter",
  "hs_total_fish_Ter",
  "hs_total_fruits_Ter",
  "hs_total_lipids_Ter",
  "hs_total_sweets_Ter",
  "hs_total_veg_Ter"
)
```

```{r final data, warning=FALSE}
combined_data_selected <- c(
  "h_bfdur_Ter",
  "hs_bakery_prod_Ter",
  "hs_break_cer_Ter",
  "hs_dairy_Ter",
  "hs_fastfood_Ter",
  "hs_org_food_Ter",
  "hs_proc_meat_Ter",
  "hs_total_fish_Ter",
  "hs_total_fruits_Ter",
  "hs_total_lipids_Ter",
  "hs_total_sweets_Ter",
  "hs_total_veg_Ter",
  "hs_cd_c_Log2",
  "hs_co_c_Log2",
  "hs_cs_c_Log2",
  "hs_cu_c_Log2",
  "hs_hg_c_Log2",
  "hs_mo_c_Log2",
  "hs_pb_c_Log2",
  "hs_dde_cadj_Log2",
  "hs_pcb153_cadj_Log2",
  "hs_pcb170_cadj_Log2",
  "hs_dep_cadj_Log2",
  "hs_pbde153_cadj_Log2",
  "hs_pfhxs_c_Log2",
  "hs_pfoa_c_Log2",
  "hs_pfos_c_Log2",
  "hs_prpa_cadj_Log2",
  "hs_mbzp_cadj_Log2",
  "hs_mibp_cadj_Log2",
  "hs_mnbp_cadj_Log2"
)

outcome_cov <- cbind(covariate_data, outcome_BMI)
outcome_cov <- outcome_cov[, !duplicated(colnames(outcome_cov))]

finalized_columns <- c(combined_data_selected)
final_selected_data <- exposome %>% dplyr::select(all_of(finalized_columns))

finalized_data <- cbind(outcome_cov, final_selected_data)
```

```{r corr plot, warning = FALSE}
numeric_finalized <- finalized_data %>%
  dplyr::select(where(is.numeric))

cor_matrix <- cor(numeric_finalized, use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 90, tl.cex = 0.6)
```

```{r correlation, warning = FALSE}
find_highly_correlated <- function(cor_matrix, threshold = 0.8) {
  cor_matrix[lower.tri(cor_matrix, diag = TRUE)] <- NA  
  cor_matrix <- as.data.frame(as.table(cor_matrix)) 
  cor_matrix <- na.omit(cor_matrix)  
  cor_matrix <- cor_matrix[order(-abs(cor_matrix$Freq)), ]  
  cor_matrix <- cor_matrix %>% filter(abs(Freq) > threshold)  
  return(cor_matrix)
}

highly_correlated_pairs <- find_highly_correlated(cor_matrix, threshold = 0.50)
highly_correlated_pairs
```

The correlation plot for the selected variables indicates notable multicollinearity among various chemical variables and the child age covariate. Using grouped regression models like LASSO, ridge, and elastic net allows for the collective handling of these highly correlated variables. This approach helps with overfitting.

LASSO (Least Absolute Shrinkage and Selection Operator) applies L1 regularization to minimize the absolute sum of coefficients. It was used to perform variable selection and regularization, effectively identifying the most significant predictors while setting less important ones to zero. Ridge regression is particularly useful for handling multicollinearity among predictors, ensuring that all variables contribute to the prediction without being overly penalized. Elastic net balances the benefits of both LASSO ridge, so by handling both variable selection and multicollinearity, elastic net is well-suited for high-dimensional datasets where predictors are correlated. Since the data gets correlated with each other when combined (adding diet, chemicals and the metabolomic serum), group LASSO, ridge, and elastic net were applied.

Decision tree (with pruning) splits the data into subsets based on the value of input features, with pruning applied to prevent overfitting. This provides an interpretable structure for understanding the relationships between variables and the outcome, though it can be prone to overfitting without pruning. Pruning gets applied to enhance generalizability. Random forest constructs multiple decision trees and merges them to obtain a more accurate and stable prediction. By averaging the predictions from numerous trees, this model reduces overfitting and captures complex interactions among variables. Gradient Boosting Machine (GBM) builds an ensemble of trees in a sequential manner, where each tree corrects the errors of its predecessors. This approach is highly effective in improving predictive accuracy by focusing on the residuals of previous trees, making it powerful for capturing non-linear relationships. These ensemble methods (razndom forest and GBM) were chosen for their robustness and high predictive accuracy. Random forest reduces variance by averaging multiple trees, while GBM improves model performance through iterative refinement.

In this study, the Root Mean Squared Error (RMSE) is used as the primary performance metric to evaluate and compare the predictive models. Using RMSE allows for straightforward comparisons between different models and datasets. Since RMSE is in the same units as the outcome variable, it facilitates direct assessment of how well different models perform in predicting BMI Z-scores, making it easier to identify the model that provides the most accurate predictions. By employing a diverse set of models, this analysis aims to identify the most significant predictors of BMI Z-scores and understand the complex interactions between dietary intake, chemical exposures, and metabolomic serum data. The combination of regularization techniques and ensemble methods ensures a comprehensive and reliable assessment of the data.

# Results

## Baseline (Covariates)

```{r baseline train/test, warning = FALSE}
covariates_selected <- c("hs_child_age_None", "e3_sex_None", "e3_yearbir_None")
baseline_data <- finalized_data %>% dplyr::select(c(covariates_selected, "hs_zbmi_who"))

x <- model.matrix(~ . -1, data = baseline_data[ , !names(baseline_data) %in% "hs_zbmi_who"])
y <- baseline_data$hs_zbmi_who
train_control <- trainControl(method = "cv", number = 10)

set.seed(101)
trainIndex <- createDataPartition(y, p = .7, list = FALSE, times = 1)
x_train <- x[trainIndex, ]
y_train <- y[trainIndex]
x_test <- x[-trainIndex, ]
y_test <- y[-trainIndex]

penalty_factors <- rep(1, ncol(x_train))
penalty_factors[colnames(x_train) %in% covariates_selected] <- 0
```

### LASSO

```{r baseline LASSO, warning = FALSE}
set.seed(101)
fit_lasso <- cv.glmnet(x_train, y_train, alpha = 1, family = "gaussian", penalty.factor = penalty_factors)
plot(fit_lasso)
coef(fit_lasso)
lasso_predictions <- predict(fit_lasso, s = "lambda.min", newx = x_test)
mse_lasso <- mean((y_test - lasso_predictions)^2)
rmse_lasso <- sqrt(mse_lasso)

cat("Baseline Lasso RMSE:", rmse_lasso, "\n")

lasso_model <- train(
  x_train, y_train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)),
  trControl = train_control,
  penalty.factor = penalty_factors
)

lasso_predictions_cv <- predict(lasso_model, x_test)
mse_lasso_cv <- mean((y_test - lasso_predictions_cv)^2)
rmse_lasso_cv <- sqrt(mse_lasso_cv)

cat("10-Fold CV Lasso RMSE:", rmse_lasso_cv)
```

The LASSO model for covariates only shows the selection of significant variables with their coefficients, with the optimal lambda chosen near log(λ) ≈ -4. This indicates the point where the model achieves the best balance between bias and variance, minimizing the mean squared error. The cross-validated RMSE is 1.152, indicating the model's predictive accuracy on unseen data. Among the covariates, only the child's age (hs_child_age_None) has a significant negative coefficient (-0.057), suggesting that as the child's age increases, the BMI Z-score slightly decreases. The other variables (sex and birth years) were not selected as significant predictors in this model. This demonstrates that age is the most influential covariate in predicting BMI Z-score, while the other covariates did not have a significant impact.

### Ridge

```{r baseline ridge, warning = FALSE}
set.seed(101)
fit_ridge <- cv.glmnet(x_train, y_train, alpha = 0, family = "gaussian", penalty.factor = penalty_factors)
plot(fit_ridge)
coef(fit_ridge)
ridge_predictions <- predict(fit_ridge, s = "lambda.min", newx = x_test)
mse_ridge <- mean((y_test - ridge_predictions)^2)
rmse_ridge <- sqrt(mse_ridge)

cat("Baseline Ridge RMSE:", rmse_ridge, "\n")

ridge_model <- train(
  x_train, y_train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 1, length = 100)),
  trControl = train_control,
  penalty.factor = penalty_factors
)

ridge_predictions_cv <- predict(ridge_model, x_test)
mse_ridge_cv <- mean((y_test - ridge_predictions_cv)^2)
rmse_ridge_cv <- sqrt(mse_ridge_cv)

cat("10-Fold CV Ridge RMSE:", rmse_ridge_cv, "\n")
```

The ridge regression model for the covariates focuses on minimizing model complexity to avoid overfitting. The mean squared error (MSE) plot against log(lambda) values shows the model's performance across various regularization levels. The optimal lambda is selected where MSE is minimized, balancing complexity and accuracy. The baseline ridge model yields an RMSE of 1.152, with a cross-validated RMSE of 1.149, indicating stable performance. The child's age (hs_child_age_None) shows a negative association with BMI Z-score, suggesting older children tend to have slightly lower BMI Z-scores. Other covariates, like sex and year of birth, have near-zero coefficients, indicating minimal impact.

### Elastic Net

```{r baseline enet, warning = FALSE}
set.seed(101)
fit_enet <- cv.glmnet(x_train, y_train, alpha = 0.5, family = "gaussian", penalty.factor = penalty_factors)
plot(fit_enet)
coef(fit_enet)
enet_predictions <- predict(fit_enet, s = "lambda.min", newx = x_test)
mse_enet <- mean((y_test - enet_predictions)^2)
rmse_enet <- sqrt(mse_enet)

cat("Baseline Elastic Net RMSE:", rmse_enet, "\n")

enet_model <- train(
  x_train, y_train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = seq(0, 1, length = 10), lambda = seq(0.001, 1, length = 100)),
  trControl = train_control,
  penalty.factor = penalty_factors
)

enet_predictions_cv <- predict(enet_model, x_test)
mse_enet_cv <- mean((y_test - enet_predictions_cv)^2)
rmse_enet_cv <- sqrt(mse_enet_cv)

cat("10-Fold CV Elastic Net RMSE:", rmse_enet_cv, "\n")
```

The elastic net model, which combines LASSO and ridge penalties for variable selection and coefficient shrinkage, achieved a baseline RMSE of 1.152. This model maintained an RMSE of 1.152 after 10-fold cross-validation, indicating stable performance. The optimal lambda was selected near log(λ) ≈ -3, suggesting minimal coefficient shrinkage. The model identified the child's age (hs_child_age_None) as a significant variable with a coefficient of -0.0574, indicating a slight negative association with the BMI Z-score. Other covariates like sex and birth year did not show significant contributions.

### Decision Tree

```{r baseline decision tree, warning = FALSE}
set.seed(101)
trainIndex <- createDataPartition(baseline_data$hs_zbmi_who, p = .7, list = FALSE, times = 1)
train_data <- baseline_data[trainIndex, ]
test_data <- baseline_data[-trainIndex, ]

fit_tree <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

rpart.plot(fit_tree)

printcp(fit_tree)
plotcp(fit_tree)

tree_predictions <- predict(fit_tree, newdata = test_data)
mse_tree <- mean((test_data$hs_zbmi_who - tree_predictions)^2)
rmse_tree <- sqrt(mse_tree)

cat("Baseline Unpruned Decision Tree RMSE:", rmse_tree, "\n")

# getting the optimal cp value that minimizes the cross-validation error
optimal_cp <- fit_tree$cptable[which.min(fit_tree$cptable[,"xerror"]), "CP"]
cat("Optimal cp value:", optimal_cp, "\n")

pruned_tree <- prune(fit_tree, cp = optimal_cp)
rpart.plot(pruned_tree)

pruned_tree_predictions <- predict(pruned_tree, newdata = test_data)
mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Baseline Pruned Decision Tree RMSE:", rmse_pruned_tree, "\n")
```

Pruning prior to cross-validation proved ineffective as it retained the full tree. However, cross-validation effectively identified the minimal cp value, ensuring the pruned tree was optimal for generalizing the model performance across unseen data. The emphasis on a single variable underscores the simplicity yet potential power of the decision tree in modeling BMI outcomes based on age.

```{r baseline decision tree cv, warning = FALSE}
set.seed(101)

train_control <- trainControl(method = "cv", number = 10)

cp_grid <- expand.grid(cp = seq(0.001, 0.1, by = 0.001))

pruned_tree_model <- train(
  hs_zbmi_who ~ .,
  data = train_data,
  method = "rpart",
  trControl = train_control,
  tuneGrid = cp_grid
)

best_cp <- pruned_tree_model$bestTune$cp
cat("Best cp value from cross-validation:", best_cp, "\n")

fit_tree_unpruned <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

fit_tree_best <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova", control = rpart.control(cp = best_cp))

rpart.plot(fit_tree_unpruned)
printcp(fit_tree_unpruned)
plotcp(fit_tree_unpruned)

rpart.plot(fit_tree_best)
printcp(fit_tree_best)
plotcp(fit_tree_best)

unpruned_tree_predictions <- predict(fit_tree_unpruned, newdata = test_data)
pruned_tree_predictions <- predict(fit_tree_best, newdata = test_data)

mse_unpruned_tree <- mean((test_data$hs_zbmi_who - unpruned_tree_predictions)^2)
rmse_unpruned_tree <- sqrt(mse_unpruned_tree)

mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Unpruned Decision Tree RMSE:", rmse_unpruned_tree, "\n")

cat("Pruned Decision Tree RMSE with Best cp:", rmse_pruned_tree, "\n")
```

The baseline decision tree model, which solely used the variable "hs_child_age_None," yielded an RMSE of 1.155. This indicates the initial model's error rate when predicting BMI Z-scores. Pruning the tree with the optimal complexity parameter (cp) of 0.0116, derived from the cross-validation, improved the RMSE to 1.149, demonstrating reduced overfitting and better performance.

After cross-validation, the optimal cp value was identified as 0.007. The pruned tree with this cp value did not further reduce the RMSE compared to the initial pruning, indicating the previous pruning was already optimal.

The pruned decision tree using cross-validation emphasizes the significance of "hs_child_age_None" in predicting BMI Z-scores. It splits the data based on whether the child's age is greater than or equal to 6.5 years. Children younger than 6.5 years have a higher BMI Z-score (0.64) compared to older children (0.34), suggesting age as a critical factor influencing BMI.

### Random Forest

```{r baseline random forest, warning = FALSE}
fit_rf <- randomForest(hs_zbmi_who ~ ., data = train_data, ntree = 500, importance = TRUE)
varImpPlot(fit_rf)
rf_predictions <- predict(fit_rf, newdata = test_data)
mse_rf <- mean((test_data$hs_zbmi_who - rf_predictions)^2)
rmse_rf <- sqrt(mse_rf)

cat("Baseline Random Forest RMSE:", rmse_rf, "\n")

rf_model <- train(
  x_train, y_train,
  method = "rf",
  trControl = train_control,
  tuneLength = 10
)

rf_predictions_cv <- predict(rf_model, x_test)
mse_rf_cv <- mean((y_test - rf_predictions_cv)^2)
rmse_rf_cv <- sqrt(mse_rf_cv)

cat("10-Fold CV Random Forest RMSE:", rmse_rf_cv, "\n")
```

For the random forest model, the RMSE of the baseline model was 1.158, and after performing 10-fold cross-validation, the RMSE slightly improved to 1.155. The model identified all three significant variables: child's age (hs_child_age_None), year of birth (e3_yearbir_None), and sex (e3_sex_None). These variables contributed the most to the model's prediction accuracy. The child's age showed the highest importance, indicating that the age of the child is the most influential factor in predicting BMI Z-score. The year of birth and sex also played notable roles but with lesser importance compared to age.

### GBM

```{r baseline GBM, warning = FALSE}
fit_gbm <- gbm(hs_zbmi_who ~ ., data = train_data, distribution = "gaussian", n.trees = 1000, interaction.depth = 3, shrinkage = 0.01, cv.folds = 5, verbose = FALSE)
summary(fit_gbm)
best_trees <- gbm.perf(fit_gbm, method = "cv")
gbm_predictions <- predict(fit_gbm, newdata = test_data, n.trees = best_trees)
mse_gbm <- mean((test_data$hs_zbmi_who - gbm_predictions)^2)
rmse_gbm <- sqrt(mse_gbm)

cat("Baseline GBM RMSE:", rmse_gbm, "\n")

gbm_model <- train(
  x_train, y_train,
  method = "gbm",
  trControl = train_control,
  tuneLength = 10,
  verbose = FALSE
)

gbm_predictions_cv <- predict(gbm_model, x_test)
mse_gbm_cv <- mean((y_test - gbm_predictions_cv)^2)
rmse_gbm_cv <- sqrt(mse_gbm_cv)

cat("10-Fold CV GBM RMSE:", rmse_gbm_cv, "\n")
```

The GBM model produced a baseline RMSE of 1.1488 and a 10-fold cross-validated RMSE of 1.1497, showing good accuracy. Key variables include child age (hs_child_age_None), year of birth (e3_yearbir_None), and sex (e3_sex_None), with child age being the most influential. The relative influence of child age (79.6) far exceeds that of birth year (14.7) and sex (5.7), indicating its dominant role in predicting BMI Z-score. The model's performance demonstrates that these covariates significantly impact BMI predictions in children.

### **Model Comparison:**

Child age consistently emerged as the most influential predictor across all models, demonstrating its strong association with BMI Z-score. While the ridge model and GBM provided slightly better performance, the choice between models may depend on the desired balance between interpretability (decision tree, LASSO) and predictive power (random forest, GBM). The ridge and elastic Net models offer a balance, retaining some influence from additional covariates, albeit minimal.

## Diet + Covariates

```{r diet + cov variables setup, warning = FALSE}
diet_selected <- c(
  "h_bfdur_Ter",
  "hs_bakery_prod_Ter",
  "hs_break_cer_Ter",
  "hs_dairy_Ter",
  "hs_fastfood_Ter",
  "hs_org_food_Ter",
  "hs_proc_meat_Ter",
  "hs_total_fish_Ter",
  "hs_total_fruits_Ter",
  "hs_total_lipids_Ter",
  "hs_total_sweets_Ter",
  "hs_total_veg_Ter"
)

covariates_selected <- c("hs_child_age_None", "e3_sex_None", "e3_yearbir_None")

diet_data <- finalized_data %>% dplyr::select(c(covariates_selected, diet_selected, "hs_zbmi_who"))

set.seed(101)
trainIndex <- createDataPartition(diet_data$hs_zbmi_who, p = .7, list = FALSE, times = 1)
train_data <- diet_data[trainIndex, ]
test_data <- diet_data[-trainIndex, ]

train_control <- trainControl(method = "cv", number = 10)

x_train <- model.matrix(hs_zbmi_who ~ . -1, train_data)
y_train <- train_data$hs_zbmi_who
x_test <- model.matrix(hs_zbmi_who ~ . -1, test_data)
y_test <- test_data$hs_zbmi_who

penalty_factors <- rep(1, ncol(x_train))
penalty_factors[colnames(x_train) %in% covariates_selected] <- 0
```

### LASSO

```{r diet + cov LASSO, warning = FALSE}
fit_lasso <- cv.glmnet(x_train, y_train, alpha = 1, family = "gaussian", penalty.factor = penalty_factors)
plot(fit_lasso)
coef(fit_lasso)
lasso_predictions <- predict(fit_lasso, s = "lambda.min", newx = x_test)
mse_lasso <- mean((y_test - lasso_predictions)^2)
rmse_lasso <- sqrt(mse_lasso)

cat("Diet + Covariates Lasso RMSE:", rmse_lasso, "\n")

lasso_model <- train(
  x_train, y_train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)),
  trControl = train_control,
  penalty.factor = penalty_factors
)

lasso_predictions_cv <- predict(lasso_model, x_test)
mse_lasso_cv <- mean((y_test - lasso_predictions_cv)^2)
rmse_lasso_cv <- sqrt(mse_lasso_cv)

cat("10-Fold CV Lasso RMSE:", rmse_lasso_cv)
```

The LASSO regression model for the combination of diet and covariates yielded an RMSE of 1.139, with a cross-validated RMSE of 1.140, indicating consistency in model performance. The optimal lambda value was determined to be near log(λ) ≈ -3, as shown in the plot. In this model, the variable child's age was the only significant variable, with a coefficient of -0.0574, indicating that as the child's age increases, the BMI Z-score decreases. This suggests that age plays a crucial role in predicting BMI Z-score in the presence of diet and covariate factors. Other variables, such as dietary intake categories, did not show significant coefficients, implying limited influence on BMI Z-score within the context of this model.

### Ridge

```{r diet + cov ridge, warning = FALSE}
fit_ridge <- cv.glmnet(x_train, y_train, alpha = 0, family = "gaussian", penalty.factor = penalty_factors)
plot(fit_ridge)
coef(fit_ridge)
ridge_predictions <- predict(fit_ridge, s = "lambda.min", newx = x_test)
mse_ridge <- mean((y_test - ridge_predictions)^2)
rmse_ridge <- sqrt(mse_ridge)

cat("Diet + Covariates Ridge RMSE:", rmse_ridge, "\n")

ridge_model <- train(
  x_train, y_train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 1, length = 100)),
  trControl = train_control,
  penalty.factor = penalty_factors
)

ridge_predictions_cv <- predict(ridge_model, x_test)
mse_ridge_cv <- mean((y_test - ridge_predictions_cv)^2)
rmse_ridge_cv <- sqrt(mse_ridge_cv)

cat("10-Fold CV Ridge RMSE:", rmse_ridge_cv, "\n")
```

For the ridge regression model, the optimal lambda was identified at approximately log(λ) = 0. The baseline RMSE for this model is 1.131, and the 10-fold cross-validated RMSE is 1.129, indicating consistent model performance. Important variables in this model include hs_child_age_None with a coefficient of -0.057, suggesting that as the age of the child increases, the BMI Z-score tends to decrease. Other dietary variables and covariates, such as various food intake categories (e.g., bakery products, dairy, fast food), did not show significant coefficients, suggesting that they have less influence in this ridge regression model for predicting BMI Z-score.

### Elastic Net

```{r diet + cov enet, warning = FALSE}
fit_enet <- cv.glmnet(x_train, y_train, alpha = 0.5, family = "gaussian", penalty.factor = penalty_factors)
plot(fit_enet)
coef(fit_enet)
enet_predictions <- predict(fit_enet, s = "lambda.min", newx = x_test)
mse_enet <- mean((y_test - enet_predictions)^2)
rmse_enet <- sqrt(mse_enet)

cat("Diet + Covariates Elastic Net RMSE:", rmse_enet, "\n")

enet_model <- train(
  x_train, y_train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = seq(0, 1, length = 10), lambda = seq(0.001, 1, length = 100)),
  trControl = train_control,
  penalty.factor = penalty_factors
)

enet_predictions_cv <- predict(enet_model, x_test)
mse_enet_cv <- mean((y_test - enet_predictions_cv)^2)
rmse_enet_cv <- sqrt(mse_enet_cv)

cat("10-Fold CV Elastic Net RMSE:", rmse_enet_cv, "\n")
```

The elastic net model's optimal lambda value, near log(λ) ≈ -3, minimizes the mean squared error. Important variables identified include "hs_child_age_None" with a coefficient of -0.057, suggesting that older age is associated with a lower BMI Z-score, while other covariates like sex and year of birth showed no significant impact. The baseline RMSE of 1.137 and the cross-validated RMSE of 1.139 indicate the model's performance and consistency.

### Decision Tree

```{r diet + cov decision tree, warning = FALSE}
fit_tree <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

rpart.plot(fit_tree)

tree_predictions <- predict(fit_tree, newdata = test_data)
mse_tree <- mean((test_data$hs_zbmi_who - tree_predictions)^2)
rmse_tree <- sqrt(mse_tree)

cat("Diet + Covariates Decision Tree RMSE:", rmse_tree, "\n")

printcp(fit_tree)
plotcp(fit_tree)

optimal_cp <- fit_tree$cptable[which.min(fit_tree$cptable[,"xerror"]), "CP"]

pruned_tree <- prune(fit_tree, cp = optimal_cp)

rpart.plot(pruned_tree)

pruned_tree_predictions <- predict(pruned_tree, newdata = test_data)
mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Pruned Diet + Covariates Decision Tree RMSE:", rmse_pruned_tree, "\n")
```

```{r}
set.seed(101)

train_control <- trainControl(method = "cv", number = 10)

cp_grid <- expand.grid(cp = seq(0.001, 0.1, by = 0.001))

pruned_tree_model <- train(
  hs_zbmi_who ~ .,
  data = train_data,
  method = "rpart",
  trControl = train_control,
  tuneGrid = cp_grid
)

best_cp <- pruned_tree_model$bestTune$cp
cat("Best cp value from cross-validation:", best_cp, "\n")

fit_tree_unpruned <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

fit_tree_best <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova", control = rpart.control(cp = best_cp))

rpart.plot(fit_tree_unpruned)
rpart.plot(fit_tree_best)

unpruned_tree_predictions <- predict(fit_tree_unpruned, newdata = test_data)
pruned_tree_predictions <- predict(fit_tree_best, newdata = test_data)

mse_unpruned_tree <- mean((test_data$hs_zbmi_who - unpruned_tree_predictions)^2)
rmse_unpruned_tree <- sqrt(mse_unpruned_tree)

mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Unpruned Decision Tree RMSE:", rmse_unpruned_tree, "\n")

cat("Pruned Decision Tree RMSE with Best cp:", rmse_pruned_tree, "\n")
```

The decision tree model for diet and covariates demonstrates that the initial unpruned model has an RMSE of 1.153, which slightly improves to 1.149 after pruning. Pruning helps by eliminating less important branches, which reduces overfitting and enhances the model's generalization capability. The complexity parameter (cp) of 0.011 was identified as the optimal value for pruning, minimizing cross-validation error. Key variables influencing the BMI Z-score include the child's age (hs_child_age_None), and dietary factors such as high consumption of bakery products (hs_bakery_prod_Ter), breakfast cereals (hs_break_cer_Ter), and lipids (hs_total_lipids_Ter). These results indicate the significant role of both age and specific dietary patterns in predicting BMI outcomes in children.

### Random Forest

```{r diet + cov random forest, warning = FALSE}
set.seed(101)
fit_rf <- randomForest(hs_zbmi_who ~ ., data = train_data)
varImpPlot(fit_rf)
importance(fit_rf)
rf_predictions <- predict(fit_rf, newdata = test_data)
mse_rf <- mean((test_data$hs_zbmi_who - rf_predictions)^2)
rmse_rf <- sqrt(mse_rf)

cat("Diet + Covariates Random Forest RMSE:", rmse_rf, "\n")

rf_model <- train(
  x_train, y_train,
  method = "rf",
  trControl = train_control,
  tuneLength = 10
)

rf_predictions_cv <- predict(rf_model, x_test)
mse_rf_cv <- mean((y_test - rf_predictions_cv)^2)
rmse_rf_cv <- sqrt(mse_rf_cv)

cat("10-Fold CV Random Forest RMSE:", rmse_rf_cv, "\n")
```

The random forest model yielded an RMSE of 1.140, with a 10-fold cross-validated RMSE of 1.129, indicating robust model performance. The most influential variable in this model was the child's age (hs_child_age_None), followed by the year of birth (e3_yearbir_None), and sex (e3_sex_None). Dietary variables also played significant roles, such as the total vegetable intake (hs_total_veg_Ter), processed meat intake (hs_proc_meat_Ter), bakery products (hs_bakery_prod_Ter), breakfast cereal intake (hs_break_cer_Ter), and fruit consumption (hs_total_fruits_Ter). These dietary factors, alongside the covariates, contributed to the prediction accuracy, highlighting the model's ability to capture the multifactorial influences on the BMI Z-score in children.

### GBM

```{r diet + cov gbm, warning = FALSE}
set.seed(101)
fit_gbm <- gbm(hs_zbmi_who ~ ., data = train_data, distribution = "gaussian", n.trees = 1000, interaction.depth = 3, shrinkage = 0.01, cv.folds = 5, verbose = FALSE)
summary(fit_gbm)
best_trees <- gbm.perf(fit_gbm, method = "cv")
gbm_predictions <- predict(fit_gbm, newdata = test_data, n.trees = best_trees)
mse_gbm <- mean((test_data$hs_zbmi_who - gbm_predictions)^2)
rmse_gbm <- sqrt(mse_gbm)

cat("Diet + Covariates GBM RMSE:", rmse_gbm, "\n")

gbm_model <- train(
  x_train, y_train,
  method = "gbm",
  trControl = train_control,
  tuneLength = 10,
  verbose = FALSE
)

gbm_predictions_cv <- predict(gbm_model, x_test)
mse_gbm_cv <- mean((y_test - gbm_predictions_cv)^2)
rmse_gbm_cv <- sqrt(mse_gbm_cv)

cat("10-Fold CV GBM RMSE:", rmse_gbm_cv, "\n")
```

The GBM modelyielded an RMSE of 1.138, with a cross-validated RMSE of 1.136, demonstrating strong predictive accuracy. The variable "hs_child_age_None" had the highest relative influence (27.745752), indicating its significant impact on the model. Other important variables included "e3_yearbir_None" (12.544772) and "hs_break_cer_Ter" (6.344034), reflecting their substantial contributions to the model's performance. The diet-related variables such as "hs_total_lipids_Ter" (5.849461), "hs_bakery_prod_Ter" (5.394027), and "hs_total_veg_Ter" (5.773139) also played crucial roles in predicting BMI Z-score, highlighting the importance between dietary habits and covariates in influencing childhood BMI outcomes.

### **Model Comparison:**

Overall, "hs_child_age_None" consistently emerged as the most influential variable across all models, highlighting its critical role in predicting BMI Z-score. Dietary variables such as "hs_bakery_prod_Ter," "hs_break_cer_Ter," and "hs_total_lipids_Ter" were also significant in multiple models, underscoring the interplay between diet and child age in influencing BMI outcomes. The GBM model, with the lowest RMSE, suggests the highest predictive accuracy among the methods evaluated.

## Chemicals + Covariates

```{r chem + cov train/test, warning = FALSE}
chemicals_selected <- c(
  "hs_cd_c_Log2", "hs_co_c_Log2", "hs_cs_c_Log2", "hs_cu_c_Log2",
  "hs_hg_c_Log2", "hs_mo_c_Log2", "hs_pb_c_Log2", "hs_dde_cadj_Log2",
  "hs_pcb153_cadj_Log2", "hs_pcb170_cadj_Log2", "hs_dep_cadj_Log2",
  "hs_pbde153_cadj_Log2", "hs_pfhxs_c_Log2", "hs_pfoa_c_Log2",
  "hs_pfos_c_Log2", "hs_prpa_cadj_Log2", "hs_mbzp_cadj_Log2", "hs_mibp_cadj_Log2",
  "hs_mnbp_cadj_Log2"
)

chemical_data <- finalized_data %>% dplyr::select(c(covariates_selected, chemicals_selected, "hs_zbmi_who"))

set.seed(101)
trainIndex <- createDataPartition(chemical_data$hs_zbmi_who, p = .7, list = FALSE, times = 1)
train_data <- chemical_data[trainIndex, ]
test_data <- chemical_data[-trainIndex, ]

train_control <- trainControl(method = "cv", number = 10)

x_train <- model.matrix(hs_zbmi_who ~ . -1, train_data)
y_train <- train_data$hs_zbmi_who
x_test <- model.matrix(hs_zbmi_who ~ . -1, test_data)
y_test <- test_data$hs_zbmi_who

penalty_factors <- rep(1, ncol(x_train))
penalty_factors[colnames(x_train) %in% covariates_selected] <- 0
```

### LASSO

```{r chem + cov lasso, warning = FALSE}
fit_lasso <- cv.glmnet(x_train, y_train, alpha = 1, family = "gaussian", penalty.factor = penalty_factors)
plot(fit_lasso)
coef(fit_lasso)
lasso_predictions <- predict(fit_lasso, s = "lambda.min", newx = x_test)
mse_lasso <- mean((y_test - lasso_predictions)^2)
rmse_lasso <- sqrt(mse_lasso)

cat("Chemical + Covariates Lasso RMSE:", rmse_lasso, "\n")

lasso_model <- train(
  x_train, y_train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)),
  trControl = train_control,
  penalty.factor = penalty_factors
)

lasso_predictions_cv <- predict(lasso_model, x_test)
mse_lasso_cv <- mean((y_test - lasso_predictions_cv)^2)
rmse_lasso_cv <- sqrt(mse_lasso_cv)

cat("10-Fold CV Lasso RMSE:", rmse_lasso_cv)
```

The LASSO regression model for chemical and covariate data includes variables such as age, sex, year of birth, and various chemical concentrations. The optimal lambda (log(λ) ≈ -3) minimizes the cross-validated error, ensuring a balance between bias and variance. The model shows a cross-validated RMSE of 1.041, indicating reasonable prediction accuracy. Important variables include hs_child_age_None (-0.04273), hs_cu_c_Log2 (0.26065), and hs_pfoa_c_Log2 (-0.0978), highlighting their significant contributions to predicting the outcome. Negative coefficients suggest an inverse relationship with the BMI Z-score, whereas positive coefficients indicate a direct relationship.

### Ridge

```{r chem + cov ridge, warning = FALSE}
fit_ridge <- cv.glmnet(x_train, y_train, alpha = 0, family = "gaussian", penalty.factor = penalty_factors)
plot(fit_ridge)
coef(fit_ridge)
ridge_predictions <- predict(fit_ridge, s = "lambda.min", newx = x_test)
mse_ridge <- mean((y_test - ridge_predictions)^2)
rmse_ridge <- sqrt(mse_ridge)

cat("Chemical + Covariates Ridge RMSE:", rmse_ridge, "\n")

ridge_model <- train(
  x_train, y_train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 1, length = 100)),
  trControl = train_control,
  penalty.factor = penalty_factors
)

ridge_predictions_cv <- predict(ridge_model, x_test)
mse_ridge_cv <- mean((y_test - ridge_predictions_cv)^2)
rmse_ridge_cv <- sqrt(mse_ridge_cv)

cat("10-Fold CV Ridge RMSE:", rmse_ridge_cv, "\n")
```

The ridge regression model for the combination of chemicals and covariates yields an RMSE of 1.036, demonstrating its predictive accuracy. The optimal lambda is chosen at a value near log(λ) ≈ -2, which balances the bias-variance tradeoff effectively. Significant variables influencing BMI Z-scores include the child's age, sex, and birth year, as well as the concentrations of various chemicals. For instance, higher levels of cadmium (hs_cd_c_Log2) and copper (hs_cu_c_Log2) show notable positive associations with BMI Z-scores. Conversely, higher concentrations of lead (hs_pb_c_Log2) and specific PCBs (hs_pcb153_cadj_Log2) are negatively associated with BMI Z-scores.

### Elastic Net

```{r chem + cov enet, warning = FALSE}
fit_enet <- cv.glmnet(x_train, y_train, alpha = 0.5, family = "gaussian", penalty.factor = penalty_factors)
plot(fit_enet)
coef(fit_enet)
enet_predictions <- predict(fit_enet, s = "lambda.min", newx = x_test)
mse_enet <- mean((y_test - enet_predictions)^2)
rmse_enet <- sqrt(mse_enet)

cat("Chemical + Covariates Elastic Net RMSE:", rmse_enet, "\n")

enet_model <- train(
  x_train, y_train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = seq(0, 1, length = 10), lambda = seq(0.001, 1, length = 100)),
  trControl = train_control,
  penalty.factor = penalty_factors
)

enet_predictions_cv <- predict(enet_model, x_test)
mse_enet_cv <- mean((y_test - enet_predictions_cv)^2)
rmse_enet_cv <- sqrt(mse_enet_cv)

cat("10-Fold CV Elastic Net RMSE:", rmse_enet_cv, "\n")
```

The elastic net model's optimal lambda, chosen around log(λ) ≈ -2.3, minimizes the mean-squared error, ensuring the best fit for the model. Significant variables included child age, with a negative coefficient indicating that older age is associated with a lower BMI Z-score. Higher log-transformed copper levels showed a strong positive association with BMI Z-score, while other chemicals such as cadmium, lead, and PCBs (polychlorinated biphenyls) displayed negative associations. The model’s baseline RMSE was 1.040, and the 10-fold cross-validated RMSE was 1.036, indicating good predictive performance. The optimal lambda helped minimize prediction error, ensuring robust model accuracy.

### Decision Tree

```{r chem + cov decision tree, warning = FALSE}
set.seed(101)
fit_tree <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

rpart.plot(fit_tree)

tree_predictions <- predict(fit_tree, newdata = test_data)
mse_tree <- mean((test_data$hs_zbmi_who - tree_predictions)^2)
rmse_tree <- sqrt(mse_tree)

cat("Chemical + Covariates Decision Tree RMSE:", rmse_tree, "\n")

printcp(fit_tree)
plotcp(fit_tree)

optimal_cp <- fit_tree$cptable[which.min(fit_tree$cptable[,"xerror"]), "CP"]

pruned_tree <- prune(fit_tree, cp = optimal_cp)

rpart.plot(pruned_tree)

pruned_tree_predictions <- predict(pruned_tree, newdata = test_data)
mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Pruned Chemical + Covariates Decision Tree RMSE:", rmse_pruned_tree, "\n")
```

```{r}
set.seed(101)

train_control <- trainControl(method = "cv", number = 10)

cp_grid <- expand.grid(cp = seq(0.001, 0.1, by = 0.001))

pruned_tree_model <- train(
  hs_zbmi_who ~ .,
  data = train_data,
  method = "rpart",
  trControl = train_control,
  tuneGrid = cp_grid
)

best_cp <- pruned_tree_model$bestTune$cp
cat("Best cp value from cross-validation:", best_cp, "\n")

fit_tree_unpruned <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

fit_tree_best <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova", control = rpart.control(cp = best_cp))
rpart.plot(fit_tree_unpruned)
rpart.plot(fit_tree_best)

unpruned_tree_predictions <- predict(fit_tree_unpruned, newdata = test_data)
pruned_tree_predictions <- predict(fit_tree_best, newdata = test_data)

mse_unpruned_tree <- mean((test_data$hs_zbmi_who - unpruned_tree_predictions)^2)
rmse_unpruned_tree <- sqrt(mse_unpruned_tree)

mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Unpruned Decision Tree RMSE:", rmse_unpruned_tree, "\n")

cat("Pruned Decision Tree RMSE with Best cp:", rmse_pruned_tree, "\n")
```

The decision tree model for predicting BMI z-scores using chemical and covariate data yielded an RMSE of 1.108, indicating the model's prediction accuracy. The primary variables used in the tree construction include log-transformed concentrations of PCB170, PBDE153, DDE, and MBZP, among others. Pruning the tree improved the RMSE to 1.090, highlighting the benefits of removing less significant branches. The optimal complexity parameter (cp) value from cross-validation was 0.023. This pruned model indicates a more generalized fit by reducing overfitting, evident from the lower cross-validated RMSE. The decision tree structure reflects the hierarchical influence of these chemical variables on BMI z-scores, with PCB170 and PBDE153 being crucial decision nodes in both the pruned and unpruned trees.

### Random Forest

```{r chem + cov random forest, warning = FALSE}
fit_rf <- randomForest(hs_zbmi_who ~ ., data = train_data, ntree = 500)
rf_predictions <- predict(fit_rf, newdata = test_data)
varImpPlot(fit_rf)
importance(fit_rf)
mse_rf <- mean((y_test - rf_predictions)^2)
rmse_rf <- sqrt(mse_rf)

cat("Chemical + Covariates Random Forest RMSE:", rmse_rf, "\n")

rf_model <- train(
  x_train, y_train,
  method = "rf",
  trControl = train_control,
  tuneLength = 10
)

rf_predictions_cv <- predict(rf_model, x_test)
mse_rf_cv <- mean((y_test - rf_predictions_cv)^2)
rmse_rf_cv <- sqrt(mse_rf_cv)

cat("10-Fold CV Random Forest RMSE:", rmse_rf_cv, "\n")
```

The random forest model shows a robust performance with an RMSE of 1.031 and a cross-validated RMSE of 1.032, indicating consistency and reliability. The model's significant variables include PCB-170, PBDE-153, and PCB-153, which have high importance in predicting the outcome. Additionally, other influential variables such as DDE, copper, and PFOA are also prominent in the model. The RMSE values suggest that the model is effective at predicting the BMI Z-score based on these chemical exposures and covariates, providing insights into the factors influencing child BMI. The minimal difference between the pruned and unpruned models indicates that pruning does not significantly enhance model performance, suggesting that the initial model is already optimized. The cross-validation further confirms the model's stability and predictive accuracy.

### GBM

```{r chem + cov gbm, warning = FALSE}
fit_gbm <- gbm(hs_zbmi_who ~ ., data = train_data, distribution = "gaussian", n.trees = 100, interaction.depth = 3, shrinkage = 0.01, cv.folds = 5)
summary(fit_gbm)
best_iter <- gbm.perf(fit_gbm, method = "cv")

gbm_predictions <- predict(fit_gbm, newdata = test_data, n.trees = best_iter)
mse_gbm <- mean((y_test - gbm_predictions)^2)
rmse_gbm <- sqrt(mse_gbm)

cat("Chemical + Covariates GBM RMSE:", rmse_gbm, "\n")

gbm_model <- train(
  x_train, y_train,
  method = "gbm",
  trControl = train_control,
  tuneLength = 10,
  verbose = FALSE
)

gbm_predictions_cv <- predict(gbm_model, x_test)
mse_gbm_cv <- mean((y_test - gbm_predictions_cv)^2)
rmse_gbm_cv <- sqrt(mse_gbm_cv)

cat("10-Fold CV GBM RMSE:", rmse_gbm_cv, "\n")
```

The GBM model's relative influence plot shows that polychlorinated biphenyl-170 (hs_pcb170_cadj_Log2), polybrominated diphenyl ether-153 (hs_pbde153_cadj_Log2), and dichlorodiphenyldichloroethylene (hs_dde_cadj_Log2) have the highest relative influence on the model's predictions. These chemicals were found to have the highest relative influence on the model's predictions, contributing to its accuracy. In terms of model performance, the cross-validated GBM showed improved predictive accuracy compared to the baseline, reflecting the robustness of the model in handling different data subsets. The GBM model has an RMSE of 1.076, and the cross-validated RMSE is 1.040, indicating good predictive accuracy and generalization to unseen data.

### **Model Comparison:**

Overall, the models for chemical and covariates data highlight the significant role of specific chemicals in predicting outcomes.  The random forest model, while also effective, showed slightly higher RMSEs. Decision tree models benefited from pruning, which improved their performance, but still lagged behind the ensemble methods. The LASSO, ridge, and elastic net models provided useful insights into variable importance but had higher RMSEs compared to the GBM, reflecting the trade-off between interpretability and predictive power. Ridge regression and elastic net provide lower RMSE values, suggesting better generalization compared to LASSO.

In summary, the models consistently identified polychlorinated biphenyl-170, polybrominated diphenyl ether-153, dichlorodiphenyldichloroethylene, copper, and perfluorooctanoic acid as crucial variables for predicting BMI Z-scores.

## Diet + Chemical + Covariates

```{r diet + chem + cov train/test, warning = FALSE}
covariates_selected <- c("hs_child_age_None", "e3_sex_None", "e3_yearbir_None")
diet_selected <- c(
  "h_bfdur_Ter", "hs_bakery_prod_Ter", "hs_break_cer_Ter", "hs_dairy_Ter",
  "hs_fastfood_Ter", "hs_org_food_Ter", "hs_proc_meat_Ter", "hs_total_fish_Ter",
  "hs_total_fruits_Ter", "hs_total_lipids_Ter", "hs_total_sweets_Ter", "hs_total_veg_Ter"
)
chemicals_selected <- c(
  "hs_cd_c_Log2", "hs_co_c_Log2", "hs_cs_c_Log2", "hs_cu_c_Log2",
  "hs_hg_c_Log2", "hs_mo_c_Log2", "hs_pb_c_Log2", "hs_dde_cadj_Log2",
  "hs_pcb153_cadj_Log2", "hs_pcb170_cadj_Log2", "hs_dep_cadj_Log2",
  "hs_pbde153_cadj_Log2", "hs_pfhxs_c_Log2", "hs_pfoa_c_Log2",
  "hs_pfos_c_Log2", "hs_prpa_cadj_Log2", "hs_mbzp_cadj_Log2", "hs_mibp_cadj_Log2",
  "hs_mnbp_cadj_Log2"
)

combined_selected <- c(covariates_selected, diet_selected, chemicals_selected)

chemical_diet_cov_data <- finalized_data %>% dplyr::select(all_of(c(combined_selected, "hs_zbmi_who")))

set.seed(101)
trainIndex <- createDataPartition(chemical_diet_cov_data$hs_zbmi_who, p = .7, list = FALSE, times = 1)
train_data <- chemical_diet_cov_data[trainIndex,]
test_data <- chemical_diet_cov_data[-trainIndex,]

train_control <- trainControl(method = "cv", number = 10)

x_train <- model.matrix(hs_zbmi_who ~ . -1, train_data)
y_train <- train_data$hs_zbmi_who
x_test <- model.matrix(hs_zbmi_who ~ . -1, test_data)
y_test <- test_data$hs_zbmi_who

penalty_factors <- rep(1, ncol(x_train))
penalty_factors[colnames(x_train) %in% covariates_selected] <- 0

num_covariates <- length(covariates_selected)
num_diet <- length(diet_selected)
num_chemicals <- length(chemicals_selected)

# make the group_indices vector
group_indices <- c(
  rep(1, num_covariates),     # Group 1: Covariates
  rep(2, num_diet),           # Group 2: Diet
  rep(3, num_chemicals)       # Group 3: Chemicals
)

# adjust length if needed
if (length(group_indices) < ncol(x_train)) {
  group_indices <- c(group_indices, rep(4, ncol(x_train) - length(group_indices)))
}
```

### Group LASSO

```{r diet + chem + cov group lasso, warning = FALSE}
set.seed(101)
group_lasso_model <- grpreg(x_train, y_train, group = group_indices, penalty = "grLasso", penalty.factor = penalty_factors, family = "gaussian")

group_lasso_predictions <- predict(group_lasso_model, x_test, type = "response")

mse_group_lasso <- mean((y_test - group_lasso_predictions)^2)
rmse_group_lasso <- sqrt(mse_group_lasso)

cat("Group Lasso RMSE:", rmse_group_lasso, "\n")
```

```{r}
set.seed(101)
cv_group_lasso <- cv.grpreg(x_train, y_train, group = group_indices, penalty = "grLasso", penalty.factor = penalty_factors, family = "gaussian", nfolds = 10)

cat("Optimal lambda:", cv_group_lasso$lambda.min, "\n")

coef_lasso <- coef(cv_group_lasso, s = "lambda.min")

sig_vars_lasso <- coef_lasso[coef_lasso != 0]
sig_vars_lasso <- sig_vars_lasso[names(sig_vars_lasso) != "(Intercept)"]

sig_vars_lasso_df <- data.frame(
  Variable = names(sig_vars_lasso),
  Coefficient = as.numeric(sig_vars_lasso)
)
sig_vars_lasso_df

group_lasso_predictions <- predict(cv_group_lasso, x_test, s = "lambda.min", type = "response")

mse_group_lasso <- mean((y_test - group_lasso_predictions)^2)
rmse_group_lasso <- sqrt(mse_group_lasso)
cat("Group LASSO RMSE with 10-fold CV:", rmse_group_lasso, "\n")
```

The cross-validated RMSE of 1.031 indicates consistent performance across different data subsets. Significant variables identified in the group LASSO model include e3_yearbir_None2009, which, with a positive coefficient of 0.349, suggests that being born in 2009 is linked to an increased BMI Z-score. Higher levels of copper (hs_cu_c_Log2) with a coefficient of 0.478 are strongly associated with an increase in BMI Z-score, while higher cadmium levels (hs_cd_c_Log2) and MiBP (hs_mibp_cadj_Log2) are linked to a decrease in BMI Z-score, with coefficients of -0.047 and -0.077, respectively. Additionally, consuming fruits within the range of 7 to 14.1 servings (hs_total_fruits_Ter(7,14.1]) is associated with an increase in BMI Z-score, as indicated by a coefficient of 0.059. These variables significantly influence the outcome based on the group LASSO model coefficients.

### Group Ridge

```{r diet + chem + cov group ridge, warning = FALSE}
set.seed(101)
group_ridge_model <- grpreg(x_train, y_train, group = group_indices, penalty = "grMCP", penalty.factor = penalty_factors, family = "gaussian")

group_ridge_predictions <- predict(group_ridge_model, x_test, type = "response")

mse_group_ridge <- mean((y_test - group_ridge_predictions)^2)
rmse_group_ridge <- sqrt(mse_group_ridge)

cat("Group Ridge RMSE:", rmse_group_ridge, "\n")
```

```{r}
set.seed(101)

cv_group_ridge_model <- cv.grpreg(x_train, y_train, group = group_indices, penalty = "grMCP", penalty.factor = penalty_factors, family = "gaussian", nfolds = 10)

cat("Optimal lambda:", cv_group_ridge_model$lambda.min, "\n")

coef_ridge <- coef(cv_group_ridge_model, s = "lambda.min")

sig_vars_ridge <- coef_ridge[coef_ridge != 0]
sig_vars_ridge <- sig_vars_ridge[names(sig_vars_ridge) != "(Intercept)"]

sig_vars_ridge_df <- data.frame(
  Variable = names(sig_vars_ridge),
  Coefficient = as.numeric(sig_vars_ridge)
)
sig_vars_ridge_df

group_ridge_predictions <- predict(cv_group_ridge_model, x_test, s = "lambda.min", type = "response")

mse_group_ridge <- mean((y_test - group_ridge_predictions)^2)
rmse_group_ridge <- sqrt(mse_group_ridge)
cat("Group Ridge RMSE with 10-fold CV:", rmse_group_ridge, "\n")
```

The group ridge model produced a RMSE of 1.042. After performing 10-fold cross-validation, the optimal lambda value identified was 0.0402, resulting in an RMSE of 1.033. Significant variables in this model included the year of birth (e3_yearbir_None2009) with a positive coefficient of 0.465, indicating that being born in 2009 is associated with an increased BMI Z-score. Copper levels (hs_cu_c_Log2) also showed a strong positive association with a coefficient of 0.606. Higher levels of mono-benzyl phthalate (hs_mbzp_cadj_Log2) were associated with an increase in the response variable, evidenced by a coefficient of 0.110. Conversely, a high intake of breakfast cereals (hs_break_cer_Ter(5.5, Inf]) was linked to a decrease in BMI Z-score with a coefficient of -0.115. Lastly, higher levels of polychlorinated biphenyl-153 (hs_pbcb153_cadj_Log2) were associated with a decrease in the response variable, indicated by a negative coefficient of -0.199.

### Group Elastic Net

```{r diet + chem + cov group enet, warning = FALSE}
set.seed(101)
group_enet_model <- grpreg(x_train, y_train, group = group_indices, penalty = "grSCAD", penalty.factor = penalty_factors, family = "gaussian")

group_enet_predictions <- predict(group_enet_model, x_test, type = "response")

mse_group_enet <- mean((y_test - group_enet_predictions)^2)
rmse_group_enet <- sqrt(mse_group_enet)

cat("Group Elastic Net RMSE:", rmse_group_enet, "\n")
```

```{r}
set.seed(101)

cv_group_enet_model <- cv.grpreg(x_train, y_train, group = group_indices, penalty = "grSCAD", penalty.factor = penalty_factors, family = "gaussian", nfolds = 10)

cat("Optimal lambda:", cv_group_enet_model$lambda.min, "\n")

coef_enet <- coef(cv_group_enet_model, s = "lambda.min")

sig_vars_enet <- coef_enet[coef_enet != 0]
sig_vars_enet <- sig_vars_enet[names(sig_vars_enet) != "(Intercept)"]

sig_vars_enet_df <- data.frame(
  Variable = names(sig_vars_enet),
  Coefficient = as.numeric(sig_vars_enet)
)
sig_vars_enet_df

group_enet_predictions <- predict(cv_group_enet_model, x_test, s = "lambda.min", type = "response")

mse_group_enet <- mean((y_test - group_enet_predictions)^2)
rmse_group_enet <- sqrt(mse_group_enet)
cat("Group Elastic Net RMSE with 10-fold CV:", rmse_group_enet, "\n")
```

The group elastic net model initially produced an RMSE of 1.043. After performing 10-fold cross-validation, the optimal lambda value identified was 0.037, resulting in an RMSE of 1.032. Significant variables in this model included the birth year 2009 (e3_yearbir_None2009) with a positive coefficient of 0.2673, indicating an association with increased BMI Z-score. Higher log-transformed copper levels (hs_cu_c_Log2) showed a strong positive association with a coefficient of 0.544. The breastfeeding duration category (h_bfdur_Ter(10.8,34.9])) was also positively associated with an increase in the outcome variable, as suggested by a coefficient of 0.087. Conversely, higher log-transformed lead levels (hs_pb_c_Log2) were negatively associated with the outcome, indicated by a coefficient of -0.096. The birth year 2004 (e3_yearbir_None2004) was negatively associated with the outcome, as evidenced by a coefficient of -0.125.

### Decision Tree

```{r diet + chem + cov decision tree, warning = FALSE}
set.seed(101)
fit_tree <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

rpart.plot(fit_tree)

tree_predictions <- predict(fit_tree, newdata = test_data)
mse_tree <- mean((test_data$hs_zbmi_who - tree_predictions)^2)
rmse_tree <- sqrt(mse_tree)

cat("Diet + Chemical + Covariates Decision Tree RMSE:", rmse_tree, "\n")

printcp(fit_tree)
plotcp(fit_tree)

optimal_cp <- fit_tree$cptable[which.min(fit_tree$cptable[,"xerror"]), "CP"]

pruned_tree <- prune(fit_tree, cp = optimal_cp)

rpart.plot(pruned_tree)

pruned_tree_predictions <- predict(pruned_tree, newdata = test_data)
mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Pruned Diet + Chemical + Covariates Decision Tree RMSE:", rmse_pruned_tree, "\n")
```

```{r diet + chem + cov decision tree cv, warning = FALSE}
set.seed(101)

train_control <- trainControl(method = "cv", number = 10)

cp_grid <- expand.grid(cp = seq(0.001, 0.1, by = 0.001))

pruned_tree_model <- train(
  hs_zbmi_who ~ .,
  data = train_data,
  method = "rpart",
  trControl = train_control,
  tuneGrid = cp_grid
)

best_cp <- pruned_tree_model$bestTune$cp
cat("Best cp value from cross-validation:", best_cp, "\n")

fit_tree_unpruned <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

fit_tree_best <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova", control = rpart.control(cp = best_cp))
rpart.plot(fit_tree_unpruned)
rpart.plot(fit_tree_best)

unpruned_tree_predictions <- predict(fit_tree_unpruned, newdata = test_data)
pruned_tree_predictions <- predict(fit_tree_best, newdata = test_data)

mse_unpruned_tree <- mean((test_data$hs_zbmi_who - unpruned_tree_predictions)^2)
rmse_unpruned_tree <- sqrt(mse_unpruned_tree)

mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Unpruned Decision Tree RMSE:", rmse_unpruned_tree, "\n")
cat("Pruned Decision Tree RMSE with Best cp:", rmse_pruned_tree, "\n")
```

The unpruned decision tree model for predicting BMI Z-score in children had an RMSE of 1.129, indicating the initial model's error rate. After applying pruning to the decision tree and with cross-validation of 10 folds, the RMSE improved to 1.090, demonstrating enhanced model performance by reducing overfitting. Pruning, which involves trimming less critical branches, helps in creating a more generalized model. Key variables identified in the decision tree include log-transformed concentrations of PCBs (hs_pcb170_cadj_Log2 and hs_pbde153_cadj_Log2), which were crucial in splitting the data. The pruned tree reveals the hierarchical influence of these variables and others such as log-transformed levels of DDE (dichlorodiphenyldichloroethylene), MiBP (Mmno-isobutyl phthalate), and PFOA (perfluorooctanoic acid) on the BMI Z-score outcomes. The best complexity parameter (cp) value from cross-validation was 0.023, indicating the optimal point at which further pruning did not improve model performance.

### Random Forest

```{r diet + chem + cov random forest, warning = FALSE}
set.seed(101)
fit_rf <- randomForest(hs_zbmi_who ~ ., data = train_data, ntree = 500)
par(mar = c(6, 14, 4, 4) + 0.1) 
varImpPlot(fit_rf, cex = 0.6)
importance(fit_rf)
rf_predictions <- predict(fit_rf, newdata = test_data)

mse_rf <- mean((y_test - rf_predictions)^2)
rmse_rf <- sqrt(mse_rf)

cat("Diet + Chemical + Covariates Random Forest RMSE:", rmse_rf, "\n")

rf_model <- train(
  x_train, y_train,
  method = "rf",
  trControl = train_control,
  tuneLength = 10
)

rf_predictions_cv <- predict(rf_model, x_test)
mse_rf_cv <- mean((y_test - rf_predictions_cv)^2)
rmse_rf_cv <- sqrt(mse_rf_cv)

cat("10-Fold CV Random Forest RMSE:", rmse_rf_cv, "\n")
```

The random forest model yielded an RMSE of 1.025 and a cross-validated RMSE of 1.026, demonstrating robust performance. Key variables contributing to this model's effectiveness include polychlorinated biphenyl-170 (hs_pcb170_cadj_Log2), polybrominated diphenyl ether-153 (hs_pbde153_cadj_Log2), dichlorodiphenyldichloroethylene (hs_dde_cadj_Log2), copper (hs_cu_c_Log2), and perfluorooctanoic acid (hs_pfoa_c_Log2). These variables were identified as having the highest importance, significantly influencing the model’s predictive accuracy by reducing prediction errors and providing valuable insights into the factors affecting childhood BMI.

### GBM

```{r diet + chem + cov gbm, warning = FALSE}
set.seed(101)
fit_gbm <- gbm(hs_zbmi_who ~ ., data = train_data, distribution = "gaussian", n.trees = 100, interaction.depth = 3, shrinkage = 0.01, cv.folds = 5)
summary(fit_gbm)
best_iter <- gbm.perf(fit_gbm, method = "cv")

gbm_predictions <- predict(fit_gbm, newdata = test_data, n.trees = best_iter)
mse_gbm <- mean((y_test - gbm_predictions)^2)
rmse_gbm <- sqrt(mse_gbm)

cat("Diet + Chemical + Covariates GBM RMSE:", rmse_gbm, "\n")

gbm_model <- train(
  x_train, y_train,
  method = "gbm",
  trControl = train_control,
  tuneLength = 10,
  verbose = FALSE
)

gbm_predictions_cv <- predict(gbm_model, x_test)
mse_gbm_cv <- mean((y_test - gbm_predictions_cv)^2)
rmse_gbm_cv <- sqrt(mse_gbm_cv)

cat("10-Fold CV GBM RMSE:", rmse_gbm_cv, "\n")
```

The RMSE for the GBM model is 1.072, with a cross-validated RMSE of 1.048, demonstrating good accuracy and improvement over iterations. Key variables in the model include polychlorinated biphenyl-170 (hs_pcb170_cadj_Log2), polybrominated diphenyl ether-153 (hs_pbde153_cadj_Log2), dichlorodiphenyldichloroethylene (hs_dde_cadj_Log2), perfluorooctanoic acid (hs_pfoa_c_Log2), and copper (hs_cu_c_Log2). These variables exhibit the highest relative influence, significantly contributing to reducing prediction errors and highlighting their importance in the model's predictive capability.

### **Model Comparison:**

The GBM model demonstrated the best overall performance with a cross-validated RMSE of 1.048, slightly better than the random forest model's cross-validated RMSE of 1.026. This suggests that GBM's iterative boosting approach may be more effective in capturing the complex relationships within the data.

Another notable observation is the consistent importance of certain variables across all models. Variables such as PCBs (hs_pcb170_cadj_Log2), PBDE-153 (hs_pbde153_cadj_Log2), DDE (hs_dde_cadj_Log2), copper (hs_cu_c_Log2), and PFOA (hs_pfoa_c_Log2) consistently emerged as significant predictors, highlighting their strong influence on childhood BMI Z-scores.

The decision tree model's pruning process and cross-validation led to a substantial reduction in overfitting, improving its performance, as shown by the decrease in RMSE from 1.129 to 1.090. This underscores the importance of model regularization techniques in enhancing predictive accuracy.

The comparison between the group LASSO, ridge, and elastic net models highlights the effectiveness of regularization techniques in handling high-dimensional data. The elastic net model, with an RMSE of 1.032 after cross-validation, effectively balanced the inclusion of significant predictors, combining the strengths of both LASSO and ridge penalties.

Overall, each model revealed significant predictors of BMI Z-scores, with environmental chemicals, dietary factors, and demographic covariates playing critical roles in influencing childhood BMI.

## Diet + Chemical + Metabolomic + Covariates

```{r adding metabolomic, warning = FALSE}
load("/Users/allison/Library/CloudStorage/GoogleDrive-aflouie@usc.edu/My Drive/HELIX_data/metabol_serum.RData")
metabol_serum_transposed <- as.data.frame(t(metabol_serum.d))
metabol_serum_transposed$ID <- as.integer(rownames(metabol_serum_transposed))

# add the ID column to the first position
metabol_serum_transposed <- metabol_serum_transposed[, c("ID", setdiff(names(metabol_serum_transposed), "ID"))] # ID is the first column, and the layout is preserved

# specific covariates
load("/Users/allison/Library/CloudStorage/GoogleDrive-aflouie@usc.edu/My Drive/HELIX_data/HELIX.RData")
filtered_chem_diet <- codebook %>%
  filter(domain %in% c("Chemicals", "Lifestyles") & period == "Postnatal" & subfamily != "Allergens")

# specific covariates
filtered_covariates <- codebook %>%
  filter(domain == "Covariates" & 
         variable_name %in% c("ID", "e3_sex_None", "e3_yearbir_None", "hs_child_age_None"))

#specific phenotype variables
filtered_phenotype <- codebook %>%
  filter(domain == "Phenotype" & 
         variable_name %in% c("hs_zbmi_who"))

# combining all necessary variables together
combined_codebook <- bind_rows(filtered_chem_diet, filtered_covariates, filtered_phenotype)

outcome_and_cov <- cbind(covariates, outcome_BMI)
outcome_and_cov <- outcome_and_cov[, !duplicated(colnames(outcome_and_cov))]
outcome_and_cov <- outcome_and_cov %>%
  dplyr::select(ID, hs_child_age_None, e3_sex_None, e3_yearbir_None, hs_zbmi_who)
#the full chemicals list
chemicals_specific <- c(
  "hs_cd_c_Log2",
  "hs_co_c_Log2",
  "hs_cs_c_Log2",
  "hs_cu_c_Log2",
  "hs_hg_c_Log2",
  "hs_mo_c_Log2",
  "hs_pb_c_Log2",
  "hs_dde_cadj_Log2",
  "hs_pcb153_cadj_Log2",
  "hs_pcb170_cadj_Log2",
  "hs_dep_cadj_Log2",
  "hs_pbde153_cadj_Log2",
  "hs_pfhxs_c_Log2",
  "hs_pfoa_c_Log2",
  "hs_pfos_c_Log2",
  "hs_prpa_cadj_Log2",
  "hs_mbzp_cadj_Log2",
  "hs_mibp_cadj_Log2",
  "hs_mnbp_cadj_Log2"
)

#postnatal diet for child
postnatal_diet <- c(
  "h_bfdur_Ter",
  "hs_bakery_prod_Ter",
  "hs_dairy_Ter",
  "hs_fastfood_Ter",
  "hs_org_food_Ter",
  "hs_readymade_Ter",
  "hs_total_bread_Ter",
  "hs_total_fish_Ter",
  "hs_total_fruits_Ter",
  "hs_total_lipids_Ter",
  "hs_total_potatoes_Ter",
  "hs_total_sweets_Ter",
  "hs_total_veg_Ter"
)

covariates_selected <- c("hs_child_age_None", "e3_sex_None", "e3_yearbir_None")

all_columns <- c(chemicals_specific, postnatal_diet)
extracted_exposome <- exposome %>% dplyr::select(all_of(all_columns))

selected_id_data <- cbind(outcome_and_cov, extracted_exposome)

# ID is the common identifier in both datasets
combined_data <- merge(selected_id_data, metabol_serum_transposed, by = "ID", all = TRUE)

selected_metabolomics_data <- combined_data %>% dplyr::select(-c(ID))
selected_metabolomics_data <- selected_metabolomics_data %>% na.omit()
```

### Group LASSO

```{r full group lasso, warning=FALSE}
set.seed(101)
trainIndex <- createDataPartition(selected_metabolomics_data$hs_zbmi_who, p = .7, list = FALSE, times = 1)
metabol_train_data <- selected_metabolomics_data[trainIndex,]
metabol_test_data  <- selected_metabolomics_data[-trainIndex,]

x_full_train <- model.matrix(hs_zbmi_who ~ . -1, metabol_train_data)
y_full_train <- metabol_train_data$hs_zbmi_who
x_full_test <- model.matrix(hs_zbmi_who ~ . -1, metabol_test_data)
y_full_test <- metabol_test_data$hs_zbmi_who

train_control <- trainControl(method = "cv", number = 10)

penalty_factors <- rep(1, ncol(x_full_train))
penalty_factors[colnames(x_full_train) %in% covariates_selected] <- 0

num_covariates <- length(covariates_selected)
num_diet <- length(diet_selected)
num_chemicals <- length(chemicals_selected)
num_metabolomics <- ncol(metabol_serum_transposed) - 1  # subtract for the ID column

group_indices <- c(
  rep(1, num_covariates),     # Group 1: Covariates
  rep(2, num_diet),           # Group 2: Diet
  rep(3, num_chemicals),      # Group 3: Chemicals
  rep(4, num_metabolomics)    # Group 4: Metabolomics
)

if (length(group_indices) < ncol(x_full_train)) {
  group_indices <- c(group_indices, rep(5, ncol(x_full_train) - length(group_indices)))
}

#length(group_indices) == ncol(x_full_train) # should be true

# fit group lasso model
set.seed(101)
group_lasso_model <- grpreg(x_full_train, y_full_train, group = group_indices, penalty = "grLasso", penalty.factor = penalty_factors, family = "gaussian")

group_lasso_predictions <- predict(group_lasso_model, x_full_test, type = "response")

mse_group_lasso <- mean((y_full_test - group_lasso_predictions)^2)
rmse_group_lasso <- sqrt(mse_group_lasso)

cat("Group Lasso Test RMSE:", rmse_group_lasso, "\n")
```

```{r group lasso cv, warning = FALSE}
set.seed(101)
cv_group_lasso <- cv.grpreg(x_full_train, y_full_train, group = group_indices, penalty = "grLasso", penalty.factor = penalty_factors, family = "gaussian", nfolds = 10)

cat("Optimal lambda:", cv_group_lasso$lambda.min, "\n")

coef_lasso <- coef(cv_group_lasso, s = "lambda.min")

sig_vars_lasso <- coef_lasso[coef_lasso != 0]
sig_vars_lasso <- sig_vars_lasso[names(sig_vars_lasso) != "(Intercept)"]

sig_vars_lasso_df <- data.frame(
  Variable = names(sig_vars_lasso),
  Coefficient = as.numeric(sig_vars_lasso)
)
sig_vars_lasso_df

group_lasso_predictions <- predict(cv_group_lasso, x_full_test, s = "lambda.min", type = "response")

mse_group_lasso <- mean((y_full_test - group_lasso_predictions)^2)
rmse_group_lasso <- sqrt(mse_group_lasso)
cat("Group LASSO RMSE with 10-fold CV:", rmse_group_lasso, "\n")
```

The group LASSO model for predicting BMI Z-score using diet, chemical, metabolomic data, and covariates produced an RMSE of 0.9334 on the test set and an improved RMSE of 0.8748 with 10-fold cross-validation. The optimal lambda for this model was found to be 0.014. The model identified several key metabolites with significant coefficients. On the positive side, metabolite 161 had the highest positive coefficient (2.8164), followed by metabolites 95 (1.3657), 88 (0.8329), 59 (0.7794), and 124 (0.7499). These positive coefficients suggest that higher levels of these metabolites are associated with an increase in BMI Z-score. Conversely, metabolite 160 had the highest negative coefficient (-3.2241), indicating a strong negative association with BMI Z-score, followed by metabolites 89 (-1.8773), 82 (-1.4959), 138 (-1.0762), and 125 (-1.0322). These negative coefficients indicate that higher levels of these metabolites are associated with a decrease in BMI Z-score.

### Group Ridge

```{r full group ridge, warning = FALSE}
set.seed(101)
group_ridge_model <- grpreg(x_full_train, y_full_train, group = group_indices, penalty = "grMCP", penalty.factor = penalty_factors, family = "gaussian")
group_ridge_predictions <- predict(group_ridge_model, x_full_test, type = "response")
mse_group_ridge <- mean((y_full_test - group_ridge_predictions)^2)
rmse_group_ridge <- sqrt(mse_group_ridge)
cat("Group Ridge RMSE:", rmse_group_ridge, "\n")
```

```{r group ridge cv, warning = FALSE}
set.seed(101)

cv_group_ridge_model <- cv.grpreg(x_full_train, y_full_train, group = group_indices, penalty = "grMCP", penalty.factor = penalty_factors, family = "gaussian", nfolds = 10)

cat("Optimal lambda:", cv_group_ridge_model$lambda.min, "\n")

coef_ridge <- coef(cv_group_ridge_model, s = "lambda.min")

sig_vars_ridge <- coef_ridge[coef_ridge != 0]
sig_vars_ridge <- sig_vars_ridge[names(sig_vars_ridge) != "(Intercept)"]

sig_vars_ridge_df <- data.frame(
  Variable = names(sig_vars_ridge),
  Coefficient = as.numeric(sig_vars_ridge)
)
sig_vars_ridge_df

group_ridge_predictions <- predict(cv_group_ridge_model, x_full_test, s = "lambda.min", type = "response")

mse_group_ridge <- mean((y_full_test - group_ridge_predictions)^2)
rmse_group_ridge <- sqrt(mse_group_ridge)
cat("Group Ridge RMSE with 10-fold CV:", rmse_group_ridge, "\n")
```

The group ridge model's optimal lambda value identified through cross-validation was 0.024, which balances the model's complexity and fit. The top five positive coefficients include metabolomic variables: metab_161, metab_95, metab_90, metab_59, and metab_177, suggesting these metabolites are positively associated with the outcome. Conversely, the top five negative coefficients, metab_160, metab_89, metab_82, metab_135, and metab_138, indicate a negative association with BMI Z-score. The model's RMSE for the test set was 0.942, and the RMSE with 10-fold cross-validation was 0.885, demonstrating good predictive performance and suggesting the inclusion of these variables captures significant variance related to BMI Z-score.

### Group Elastic Net

```{r full group enet, warning = FALSE}
set.seed(101)
group_enet_model <- grpreg(x_full_train, y_full_train, group = group_indices, penalty = "grSCAD", penalty.factor = penalty_factors, family = "gaussian")
group_enet_predictions <- predict(group_enet_model, x_full_test, type = "response")
mse_group_enet <- mean((y_full_test - group_enet_predictions)^2)
rmse_group_enet <- sqrt(mse_group_enet)
cat("Group Elastic Net RMSE:", rmse_group_enet, "\n")
```

```{r group enet cv, warning = FALSE}
set.seed(101)

cv_group_enet_model <- cv.grpreg(x_full_train, y_full_train, group = group_indices, penalty = "grSCAD", penalty.factor = penalty_factors, family = "gaussian", nfolds = 10)

cat("Optimal lambda:", cv_group_enet_model$lambda.min, "\n")

coef_enet <- coef(cv_group_enet_model, s = "lambda.min")

sig_vars_enet <- coef_enet[coef_enet != 0]
sig_vars_enet <- sig_vars_enet[names(sig_vars_enet) != "(Intercept)"]

sig_vars_enet_df <- data.frame(
  Variable = names(sig_vars_enet),
  Coefficient = as.numeric(sig_vars_enet)
)
sig_vars_enet_df

group_enet_predictions <- predict(cv_group_enet_model, x_full_test, s = "lambda.min", type = "response")

mse_group_enet <- mean((y_full_test - group_enet_predictions)^2)
rmse_group_enet <- sqrt(mse_group_enet)
cat("Group Elastic Net RMSE with 10-fold CV:", rmse_group_enet, "\n")
```

The group elastic net model demonstrated an RMSE of 0.9441 on the test set, which improved to 0.8845 with 10-fold cross-validation, indicating a strong predictive performance. The optimal lambda value was 0.0185. The top five positive coefficients, reflecting variables with the strongest positive associations, included metabolomic variables metab_161, metab_95, metab_90, metab_59, and metab_177, with metab_161 showing the highest positive coefficient of 3.6039. Conversely, the variables metab_160, metab_89, metab_82, metab_135, and metab_138 had the most substantial negative associations, with metab_160 exhibiting the largest negative coefficient of -4.0738. These results suggest that these specific metabolites have significant associations with the BMI Z-scores, either increasing or decreasing them, which could provide insights into the biological pathways influencing BMI in children.

### Decision Trees

```{r full decision tree, warning = FALSE}
selected_metabolomics_data <- selected_metabolomics_data %>% na.omit()

set.seed(101)
trainIndex <- createDataPartition(selected_metabolomics_data$hs_zbmi_who, p = .7, 
                                  list = FALSE, 
                                  times = 1)
train_data <- selected_metabolomics_data[ trainIndex,]
test_data  <- selected_metabolomics_data[-trainIndex,]

x_full_train <- model.matrix(hs_zbmi_who ~ ., train_data)[,-1]
y_full_train <- train_data$hs_zbmi_who
x_full_test <- model.matrix(hs_zbmi_who ~ ., test_data)[,-1]
y_full_test <- test_data$hs_zbmi_who

set.seed(101)
fit_tree <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

rpart.plot(fit_tree)

tree_predictions <- predict(fit_tree, newdata = test_data)
mse_tree <- mean((test_data$hs_zbmi_who - tree_predictions)^2)
rmse_tree <- sqrt(mse_tree)

cat("Full Decision Tree RMSE:", rmse_tree, "\n")

printcp(fit_tree)
plotcp(fit_tree)

optimal_cp <- fit_tree$cptable[which.min(fit_tree$cptable[,"xerror"]), "CP"]

pruned_tree <- prune(fit_tree, cp = optimal_cp)

rpart.plot(pruned_tree)

pruned_tree_predictions <- predict(pruned_tree, newdata = test_data)
mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Pruned Decision Tree RMSE:", rmse_pruned_tree, "\n")
```

```{r full decision tree cv, warning = FALSE}
set.seed(101)

train_control <- trainControl(method = "cv", number = 10)

cp_grid <- expand.grid(cp = seq(0.001, 0.1, by = 0.001))

pruned_tree_model <- train(
  hs_zbmi_who ~ .,
  data = train_data,
  method = "rpart",
  trControl = train_control,
  tuneGrid = cp_grid
)

best_cp <- pruned_tree_model$bestTune$cp
cat("Best cp value from cross-validation:", best_cp, "\n")

fit_tree_unpruned <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova")

fit_tree_best <- rpart(hs_zbmi_who ~ ., data = train_data, method = "anova", control = rpart.control(cp = best_cp))
rpart.plot(fit_tree_unpruned)
rpart.plot(fit_tree_best)

unpruned_tree_predictions <- predict(fit_tree_unpruned, newdata = test_data)
pruned_tree_predictions <- predict(fit_tree_best, newdata = test_data)

mse_unpruned_tree <- mean((test_data$hs_zbmi_who - unpruned_tree_predictions)^2)
rmse_unpruned_tree <- sqrt(mse_unpruned_tree)

mse_pruned_tree <- mean((test_data$hs_zbmi_who - pruned_tree_predictions)^2)
rmse_pruned_tree <- sqrt(mse_pruned_tree)

cat("Unpruned Decision Tree RMSE:", rmse_unpruned_tree, "\n")
cat("Pruned Decision Tree RMSE with Best cp:", rmse_pruned_tree, "\n")
```

The key variables identified include polychlorinated biphenyl-170 (hs_pcb170_cadj_Log2), metab_95, metab_129, and several metabolomics variables such as metab_161, metab_140, and metab_176. The unpruned model had an RMSE of 1.243, while the pruned model, which used the optimal cp value from cross-validation, had an RMSE of 1.128. This indicates that pruning improved the model's accuracy by reducing overfitting, resulting in better performance on the test data.

### Random Forest

```{r rand forest predictions, warning = FALSE}
set.seed(101)
rf_model <- randomForest(hs_zbmi_who ~ . , data = train_data, ntree = 500)
importance(rf_model)
par(mar = c(6, 14, 4, 4) + 0.1) 
varImpPlot(rf_model, cex = 0.6)

rf_predictions <- predict(rf_model, newdata = test_data)

rf_mse <- mean((rf_predictions - y_test)^2)
rmse_rf <- sqrt(rf_mse)

cat("Diet + Chemical + Metabolomic + Covariates Random Forest RMSE:", rmse_rf, "\n")

rf_model <- train(
  x_full_train, y_full_train,
  method = "rf",
  trControl = train_control,
  tuneLength = 10
)

rf_predictions_cv <- predict(rf_model, x_full_test)
mse_rf_cv <- mean((y_full_test - rf_predictions_cv)^2)
rmse_rf_cv <- sqrt(mse_rf_cv)

cat("10-Fold CV Random Forest RMSE:", rmse_rf_cv, "\n")
```

The IncNodePurity values indicate that "hs_pcb170_cadj_Log2" is the most influential variable, followed by "metab_95," "hs_pcb153_cadj_Log2," and "metab_8." The model has an RMSE of 1.0047 for the training set and 1.0048 for the 10-fold cross-validation, suggesting a consistent performance with minimal overfitting. Comparing the pruned and unpruned models, the pruned model with cross-validation resulted in a lower RMSE, indicating better generalization and reduced complexity without sacrificing accuracy. 

### GBM

```{r gbm fit, warning = FALSE}
gbm_model_full <- gbm(hs_zbmi_who ~ ., data = train_data, 
                 distribution = "gaussian",
                 n.trees = 1000,
                 interaction.depth = 3,
                 n.minobsinnode = 10,
                 shrinkage = 0.01,
                 cv.folds = 5,
                 verbose = TRUE)
summary(gbm_model_full)

# finding the best number of trees based on cross-validation
best_trees_full <- gbm.perf(gbm_model_full, method = "cv")
predictions_gbm_full <- predict(gbm_model_full, test_data, n.trees = best_trees_full)
mse_gbm_full <- mean((y_full_test - predictions_gbm_full)^2)
rmse_gbm_full <- sqrt(mse_gbm_full)

cat("Diet + Chemical + Metabolomic + Covariates GBM RMSE:", rmse_gbm_full, "\n")

gbm_model_full <- train(
  x_full_train, y_full_train,
  method = "gbm",
  trControl = train_control,
  tuneLength = 10,
  verbose = FALSE
)

gbm_predictions_cv <- predict(gbm_model_full, x_full_test)
mse_gbm_cv <- mean((y_full_test - gbm_predictions_cv)^2)
rmse_gbm_cv <- sqrt(mse_gbm_cv)

cat("10-Fold CV GBM RMSE:", rmse_gbm_cv, "\n")
```

The GBM model shows that the most influential variable is polychlorinated biphenyl-170 (hs_pcb170_cadj_Log2), followed by metab_95, metab_161, metab_8, and metab_49. The GBM model's RMSE is 0.954, indicating good predictive performance. The cross-validated RMSE is 0.953, showing consistent performance across different folds. The variables polychlorinated biphenyl-170 and metab_95 have the highest relative influence, suggesting significant roles in predicting the outcome.

### **Model Comparison:**

In the model comparison for the diet, chemical, metabolomic, and covariates dataset, the GBM performed the best with an RMSE of 0.9537, closely followed by the group LASSO model with an RMSE of 0.8748. The random forest model also showed strong performance with an RMSE of 1.0047. Key variables that consistently appeared across the models as important predictors include specific metabolites (metab_161, metab_95, metab_160, metab_8) and chemical exposure variables (hs_pcb170_cadj_Log2, hs_pbde153_cadj_Log2). These variables highlight the critical influence of specific PCB levels and metabolites in predicting health outcomes.

# Conclusion

## Overall Findings

Throughout the analysis, specific variables consistently emerged as significant predictors of BMI Z-scores in children. Variables such as polychlorinated biphenyl-170 (hs_pcb170_cadj_Log2), polybrominated diphenyl ether-153 (hs_pbde153_cadj_Log2), and various metabolites (metab_161, metab_95, metab_160) were highlighted across multiple models, indicating their strong influence on the outcome. Notably, higher levels of copper (hs_cu_c_Log2) and certain metabolites (metab_161, metab_95) were consistently associated with an increased BMI Z-score, suggesting their role in contributing to higher BMI. Conversely, variables such as cadmium (hs_cd_c_Log2) and specific metabolites (metab_160, metab_89) showed negative associations with BMI Z-score, indicating a potential protective effect against increased BMI. These findings imply that both environmental chemical exposures and specific metabolic profiles play crucial roles in childhood BMI outcomes, emphasizing the need for comprehensive interventions targeting these factors to mitigate obesity risk and promote healthier growth trajectories.

The LASSO, ridge, and elastic net models consistently demonstrated strong performance across various datasets, achieving the lowest RMSEs, particularly when including diet, chemical, metabolomic, and covariates. The incorporation of metabolomic data significantly enhanced model performance, underscoring their predictive power concerning BMI. Decision trees generally exhibited moderate performance, with pruning enhancing RMSE values. Nonetheless, the full decision tree models were less effective compared to regularized regression models. Random Forest and GBM models exhibited robust performance, with GBM marginally surpassing Random Forest. The integration of diverse data types (diet, chemical, metabolomic) led to improved RMSE values, showcasing the advantage of a comprehensive data approach.

Among the models tested, the GBM with all variables exhibited the best overall performance with an RMSE of 0.9537, closely followed by the group LASSO model with an RMSE of 0.8748, demonstrating superior predictive accuracy and generalization capabilities.

The results imply that certain environmental chemicals and metabolites play crucial roles in influencing BMI Z-scores in children. The high predictive accuracy of models like GBM and group LASSO underscores the importance of these variables. Higher levels of certain metabolites and chemical exposures were associated with significant changes in BMI Z-scores, suggesting potential areas for intervention to manage or mitigate obesity in children. The findings indicate that environmental and dietary factors, alongside inherent covariates like age, have substantial impacts on children's health outcomes, specifically BMI.

## Limitations & Future

One of the limitations of this study is the exclusion of 103 rows with missing values from the combined dataset, which represents approximately 8% of the total data. This decision was made to ensure the integrity and robustness of our analysis, as many statistical methods and machine learning algorithms require complete data. However, this exclusion may have introduced some issues. The excluded rows might have specific characteristics that differ from the rest of the dataset. As a result, the cleaned dataset may not fully capture the diversity and variability of the original population, potentially limiting the representativeness of the study's findings. In future studies, alternative methods such as imputation techniques could be explored to handle missing data, potentially preserving more information and reducing the risk of bias.

Some covariates related to pregnancy, such as sex and year of birth, were included in the analysis as they were considered potentially relevant. However, their direct relevance to the specific outcomes studied might be limited, which could introduce some noise or confounding effects. There may have been other variables that might have been relevant but have been overlooked in this study. So for future studies, it might be possible to look at the impact of other potential covariates and confounders on the outcomes of interest.

# References

Caspersen, I. H. et al. (2016). Determinants of persistent organic pollutant concentrations in plasma from 300 Norwegian women and their 3-year-old children. Environmental Research, 146, 18-26. DOI: [10.1016/j.envres.2015.12.008](https://doi.org/10.1016/j.envres.2015.12.008){.uri}

Junge, K. M. et al. (2018). MEST mediates the impact of prenatal bisphenol A exposure on long-term body weight development. Clinical Epigenetics, 10(1), 58. DOI: [10.1186/s13148-018-0478-z](https://doi.org/10.1186/s13148-018-0478-z){.uri}

Harley, K. G. et al. (2013). Association of prenatal and childhood PBDE and PCB exposures with timing of puberty in boys and girls. Environmental International, 51, 264-272. DOI: [10.1016/j.envint.2012.10.007](https://doi.org/10.1016/j.envint.2012.10.007){.uri}

# Appendices

## Selected Diet Variable Summary

```{r categorical diet, warning=FALSE}
categorical_diet <- dietary_exposome %>% 
  dplyr::select(where(is.factor))

categorical_diet_long <- pivot_longer(
  categorical_diet,
  cols = everything(),
  names_to = "variable",
  values_to = "value"
)

unique_categorical_vars <- unique(categorical_diet_long$variable)
categorical_plots <- lapply(unique_categorical_vars, function(var) {
  data <- filter(categorical_diet_long, variable == var)
  
  p <- ggplot(data, aes(x = value, fill = value)) +
    geom_bar(stat = "count") +
    labs(title = paste("Distribution of", var), x = var, y = "Count")
  
  print(p)
  return(p)
})
```

Breastfeeding Duration: Majority of observations are in the highest duration category, suggesting longer breastfeeding periods are common.

Bakery Products: Shows a relatively even distribution across the three categories, indicating varied consumption levels of bakery products among participants.

Breakfast Cereal: The highest category of cereal consumption is the most common, suggesting a preference for or greater consumption of cereals.

Dairy: Shows a fairly even distribution across all categories, indicating a uniform consumption pattern of dairy products.

Fast Food: Most participants fall into the middle category, indicating moderate consumption of fast food.

Organic Food: Most participants either consume a lot of or no organic food, with fewer in the middle range.

Processed Meat: Consumption levels are fairly evenly distributed, indicating varied dietary habits regarding processed meats.

Bread: Distribution shows a significant leaning towards higher bread consumption.

Cereal: Even distribution across categories suggests varied cereal consumption habits.

Fish and Seafood: Even distribution across categories, indicating varied consumption of fish and seafood.

Fruits: High fruit consumption is the most common, with fewer participants in the lowest category.

Added Fats: More participants consume added fats at the lowest and highest levels, with fewer in the middle.

Sweets: High consumption of sweets is the most common, indicating a preference for or higher access to sugary foods.

Vegetables: Most participants consume a high amount of vegetables.

## Selected Chemical Variable Summary

```{r numeric chemicals, warning=FALSE}
#separate numeric and categorical data
numeric_chemical <- chemical_exposome %>% 
  dplyr::select(where(is.numeric))

numeric_chemical_long <- pivot_longer(
  numeric_chemical,
  cols = everything(),
  names_to = "variable",
  values_to = "value"
)

unique_numerical_vars <- unique(numeric_chemical_long$variable)

num_plots <- lapply(unique_numerical_vars, function(var) {
  data <- filter(numeric_chemical_long, variable == var)
  p <- ggplot(data, aes(x = value)) +
    geom_histogram(bins = 30, fill = "blue") +
    labs(title = paste("Histogram of", var), x = "Value", y = "Count")
  print(p)
  return(p)
})
```

Cadmium (hs_cd_c_Log2): The histogram for cadmium exposure shows a relatively symmetric distribution centered around 4 on the log2 scale. Most values range from approximately 3 to 5, with few outliers.

Cobalt (hs_co_c_Log2): The histogram of cobalt levels displays a roughly normal distribution centered around a slight positive skew, peaking around 3.5.

Cesium (hs_cs_c_Log2): Exhibits a right-skewed distribution, indicating that most participants have relatively low exposure levels, but a small number have substantially higher exposures. Majority of the data centered around 1.5 to 2.5

Copper (hs_cu_c_Log2): Shows a right-skewed distribution, suggesting that while most individuals have moderate exposure, a few experience significantly higher levels of copper.

Mercury (hs_hg_c_Log2): This distribution is also right-skewed, common for environmental pollutants, where a majority have lower exposure levels, and a minority have high exposure levels.

Molybdenum (hs_mo_c_Log2): Shows a distribution with a sharp peak and a long right tail, suggesting that while most people have similar exposure levels, a few have exceptionally high exposures. Has a sharp peak around 6, indicating that most values fall within a narrow range.

Lead (hs_pb_c_Log2): The distribution is slightly right-skewed, indicating higher exposure levels in a smaller group of the population compared to the majority.

DDE (hs_dde_cadj_Log2): Shows a pronounced right skew, typical for chemicals that accumulate in the environment and in human tissues, indicating higher levels of exposure in a smaller subset of the population..

PCB 153 (hs_pcb153_cadj_Log2): Has a distribution with right skewness, suggesting that exposure to these compounds is higher among a smaller segment of the population. Bimodal, indicating two peaks around 2 and 4.

PCB 170 (hs_pcb170_cadj_Log2): This histograms show a significant right skew, indicating lower concentrations of these chemicals in most samples, with fewer samples showing higher concentrations. This pattern suggests that while most individuals have low exposure, a few may have considerably higher levels.

DEP (hs_dep_cadj_Log2): DEP exposure has a sharp peak around 6, indicating a narrow distribution of values.

PBDE 153 (hs_pbde153_cadj_Log2): This histogram shows a bimodal distribution, with peaks around 1 and 4.

PFHxS: Distribution peaks around 5 with a broad spread, indicating variation in PFHxS levels.

PFOA: Shows a peak around 6 with a symmetrical distribution, indicating consistent PFOA levels.

PFOS: Similar to PFOA, the distribution peaks around 6, indicating consistent PFOS levels.

Propyl Paraben (hs_prpa_cadj_Log2): The distribution peaks around 6 with a broad spread, indicating variability in propyl paraben levels.

Monobenzyl Phthalate (hs_mbzp_cadj_Log2): This histogram shows a right-skewed distribution. Most values cluster at the lower end, indicating a common lower exposure level among subjects, with a long tail towards higher values suggesting occasional higher exposures. Shows a broad distribution with a peak around 4, indicating variation in monobenzyl phthalate levels. Indicates consistent but varied exposure levels.

Monoisobutyl Phthalate (hs_mibp_cadj_Log2): The distribution is right-skewed, similar to MBZP, but with a smoother decline. This pattern also indicates that while most subjects have lower exposure levels, a few experience significantly higher exposures.

Mono-n-butyl Phthalate (hs_mnbp_cadj_Log2): Peaks around 4, indicating consistent exposure levels with some variation. Few outliers are present.

## Selected Covariate Variable Summary

```{r covariates numeric, warning=FALSE}
#separate numeric and categorical data
numeric_covariates <- covariate_data %>% 
  dplyr::select(where(is.numeric))

numeric_covariates_long <- pivot_longer(
  numeric_covariates,
  cols = everything(),
  names_to = "variable",
  values_to = "value"
)

unique_numerical_vars <- unique(numeric_covariates_long$variable)

num_plots <- lapply(unique_numerical_vars, function(var) {
  data <- filter(numeric_covariates_long, variable == var)
  p <- ggplot(data, aes(x = value)) +
    geom_histogram(bins = 30, fill = "blue") +
    labs(title = paste("Histogram of", var), x = "Value", y = "Count")
  print(p)
  return(p)
})
```

Child's Age (hs_child_age): This histogram is multimodal, reflecting several peaks across different ages. This could be indicative of the data collection points or particular age groups being studied.

```{r covariates categorical, warning=FALSE}
categorical_covariates <- covariate_data %>% 
  dplyr::select(where(is.factor))

categorical_covariates_long <- pivot_longer(
  categorical_covariates,
  cols = everything(),
  names_to = "variable",
  values_to = "value"
)

unique_categorical_vars <- unique(categorical_covariates_long$variable)
categorical_plots <- lapply(unique_categorical_vars, function(var) {
  data <- filter(categorical_covariates_long, variable == var)
  
  p <- ggplot(data, aes(x = value, fill = value)) +
    geom_bar(stat = "count") +
    labs(title = paste("Distribution of", var), x = var, y = "Count")
  
  print(p)
  return(p)
})
```

Gender Distribution (e3_sex): The gender distribution is nearly balanced with a slight higher count for males compared to females.

Year of Birth (e3_yearbir): This chart shows that the majority of subjects were born in the later years, with a significant increase in 2009, indicating perhaps a larger recruitment or a specific cohort focus that year.

## Full Model Gradient Boosting Machine Top 5 Variables

```{r full gbm model top 5 visual, warning = FALSE}
# summary of the model to extract feature importance
importance_full <- summary(gbm_model_full, n.trees = best_trees)

#data frame for the top 5 features
top5_importance_full <- importance_full[1:5, ]

ggplot(top5_importance_full, aes(x = reorder(var, rel.inf), y = rel.inf)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 5 Most Important Features in GBM Model",
       x = "Feature",
       y = "Relative Importance")
```

## Overall Comparison of RMSE

```{r OVERALL, warning = FALSE}
results <- data.frame(
  Model = rep(c("Lasso", "Ridge", "Elastic Net", "Decision Tree", "Random Forest", "GBM"), each = 5),
  Data_Set = rep(c("Covariates", "Diet + Covariates", "Chemicals + Covariates", "Diet + Chemical + Covariates", "Diet + Chemical + Metabolomic + Covariates"), 6),
  RMSE = c(
    1.152017, 1.139468, 1.040727, 1.031241, 0.8748226, # Lasso
    1.149128, 1.129036, 1.035411, 1.03338, 0.8849928,  # Ridge
    1.152017, 1.138767, 1.035456, 1.032168, 0.8844928, # Elastic Net
    1.155427, 1.152094, 1.089746, 1.089746, 1.127666, # Decision Tree
    1.154789, 1.129478, 1.031669, 1.025544, 1.004854, # Random Forest
    1.149695, 1.136542, 1.040439, 1.048183, 0.9526345  # GBM
  )
)

results$Data_Set <- factor(results$Data_Set, levels = c("Covariates", "Diet + Covariates", "Chemicals + Covariates", "Diet + Chemical + Covariates", "Diet + Chemical + Metabolomic + Covariates"))

rmse_plot <- ggplot(results, aes(x = Data_Set, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Model RMSE Comparison", x = "Data Set", y = "Root Mean Squared Error") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

rmse_plot
```

## Comparison of LASSO Models' RMSE

```{r LASSO, warning = FALSE}
# filter results for Lasso model
lasso_results <- subset(results, Model == "Lasso")

rmse_lasso_plot <- ggplot(lasso_results, aes(x = Data_Set, y = RMSE, fill = Data_Set)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Lasso Model RMSE Comparison", x = "Data Set", y = "Root Mean Squared Error") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

rmse_lasso_plot
```

## Comparison of Ridge Models' RMSE

```{r RIDGE, warning = FALSE}
# filter results for ridge model
ridge_results <- subset(results, Model == "Ridge")

rmse_ridge_plot <- ggplot(ridge_results, aes(x = Data_Set, y = RMSE, fill = Data_Set)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Ridge Model RMSE Comparison", x = "Data Set", y = "Root Mean Squared Error") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

rmse_ridge_plot
```

## Comparison of Elastic Net Models' RMSE

```{r ENET, warning = FALSE}
# filter results for elastic net model
enet_results <- subset(results, Model == "Elastic Net")

rmse_enet_plot <- ggplot(enet_results, aes(x = Data_Set, y = RMSE, fill = Data_Set)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Elastic Net Model RMSE Comparison", x = "Data Set", y = "Root Mean Squared Error") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

rmse_enet_plot
```

## Comparison of Decision Tree Models' RMSE

```{r DECISION TREE, warning = FALSE}
tree_results <- data.frame(
  Data_Set = c("Covariates", "Diet + Covariates", "Chemicals + Covariates", "Diet + Chemical + Covariates", "Diet + Chemical + Metabolomic + Covariates"),
  RMSE = c(1.155427, 1.152094, 1.089746, 1.089746, 1.127666)
)

tree_results$Data_Set <- factor(tree_results$Data_Set, levels = c("Covariates", "Diet + Covariates", "Chemicals + Covariates", "Diet + Chemical + Covariates", "Diet + Chemical + Metabolomic + Covariates"))

rmse_tree_plot <- ggplot(tree_results, aes(x = Data_Set, y = RMSE, fill = Data_Set)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Decision Tree Model RMSE Comparison", x = "Data Set", y = "Root Mean Squared Error") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

rmse_tree_plot
```

## Comparison of Random Forest Models' RMSE

```{r RANDOM FOREST, warning = FALSE}
# filter results for random forest model
rf_results <- data.frame(
  Data_Set = c("Covariates", "Diet + Covariates", "Chemicals + Covariates", "Diet + Chemical + Covariates", "Diet + Chemical + Metabolomic + Covariates"),
  RMSE = c(1.154789, 1.129478, 1.031669, 1.025544, 1.004854)
)

rf_results$Data_Set <- factor(tree_results$Data_Set, levels = c("Covariates", "Diet + Covariates", "Chemicals + Covariates", "Diet + Chemical + Covariates", "Diet + Chemical + Metabolomic + Covariates"))

rmse_rf_plot <- ggplot(rf_results, aes(x = Data_Set, y = RMSE, fill = Data_Set)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Random Forest Model RMSE Comparison", x = "Data Set", y = "Root Mean Squared Error") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

rmse_rf_plot
```

## Comparison of Gradient Boost Machine Models' RMSE

```{r GBM, warning = FALSE}
# filter results for elastic net model
gbm_results <- data.frame(
  Data_Set = c("Covariates", "Diet + Covariates", "Chemicals + Covariates", "Diet + Chemical + Covariates", "Diet + Chemical + Metabolomic + Covariates"),
  RMSE = c(1.149695, 1.136542, 1.040439, 1.048183, 0.9526345)
)

gbm_results$Data_Set <- factor(gbm_results$Data_Set, levels = c("Covariates", "Diet + Covariates", "Chemicals + Covariates", "Diet + Chemical + Covariates", "Diet + Chemical + Metabolomic + Covariates"))

rmse_gbm_plot <- ggplot(gbm_results, aes(x = Data_Set, y = RMSE, fill = Data_Set)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Gradient Boost Machine Model RMSE Comparison", x = "Data Set", y = "Root Mean Squared Error") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

rmse_gbm_plot
```
